{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KddCup-AnomallyDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Dataset saved as fullDataNormalized.csv file\n",
    "\n",
    "Normalized Trainingset saved as trainingSetNormalized.csv file\n",
    "\n",
    "Normalized Testset saved as testSetNormalized.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingSetNormalized.csv file opened\n",
      "testSetNormalized.csv file opened\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "with open('trainingSetNormalized.csv', 'rb') as f1:\n",
    "    reader = csv.reader(f1)\n",
    "    trainList = list(reader)\n",
    "print(\"trainingSetNormalized.csv file opened\")\n",
    "    \n",
    "with open('testSetNormalized.csv', 'rb') as f2:\n",
    "    reader = csv.reader(f2)\n",
    "    testList = list(reader)    \n",
    "print(\"testSetNormalized.csv file opened\")\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total length of train data : ', 787953)\n",
      "('Total length of test data : ', 247256)\n"
     ]
    }
   ],
   "source": [
    "def file_len(adress):\n",
    "    f = open(adress)\n",
    "    nr_of_lines = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return nr_of_lines\n",
    "\n",
    "trainLen = file_len('trainingSetNormalized.csv')\n",
    "print(\"Total length of train data : \", trainLen)\n",
    "\n",
    "testLen = file_len('testSetNormalized.csv')\n",
    "print(\"Total length of test data : \", testLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TrainingSet : ', (787953, 41))\n",
      "('TestSet : ', (247256, 41))\n"
     ]
    }
   ],
   "source": [
    "trainingSet = np.empty(shape=[trainLen, 41])\n",
    "testSet = np.empty(shape=[testLen, 41]) \n",
    "\n",
    "for i in range(trainLen):\n",
    "    for j in range(41):\n",
    "        trainingSet[i][j] = trainList[i][j]\n",
    "print(\"TrainingSet : \", trainingSet.shape)     \n",
    "for i in range(trainLen):\n",
    "    trainingSet[i][19] = 0\n",
    "\n",
    "for i in range(testLen):\n",
    "    for j in range(41):\n",
    "        testSet[i][j] = testList[i][j] \n",
    "for i in range(testLen):\n",
    "    testSet[i][19] = 0\n",
    "print(\"TestSet : \", testSet.shape)        \n",
    "#index 19 is nan because std is 0,,, so in matrix' 19th index element = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 40 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3939)\n",
      "('Epoch:', '0001', 'cost=', '1.773986101')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0301', 'cost=', '0.871800661')\n",
      "('Step Count :', 300)\n",
      "('Epoch:', '0601', 'cost=', '0.130277842')\n",
      "('Step Count :', 600)\n",
      "('Epoch:', '0901', 'cost=', '0.117659949')\n",
      "('Step Count :', 900)\n",
      "('Epoch:', '1201', 'cost=', '0.098911457')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1501', 'cost=', '0.116090700')\n",
      "('Step Count :', 1500)\n",
      "('Epoch:', '1801', 'cost=', '0.104804605')\n",
      "('Step Count :', 1800)\n",
      "('Epoch:', '2101', 'cost=', '0.109346196')\n",
      "('Step Count :', 2100)\n",
      "('Epoch:', '2401', 'cost=', '0.821393967')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2701', 'cost=', '0.105472445')\n",
      "('Step Count :', 2700)\n",
      "('Epoch:', '3001', 'cost=', '0.095400371')\n",
      "('Step Count :', 3000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 300\n",
    "num_steps=3001\n",
    "total_batch = int(trainingSet.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainingSet.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainingSet[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: testSet[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 41)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_decode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.67600000e-02,  -3.77861000e+00,  -2.48153000e+00,\n",
       "        -5.50310000e-01,  -2.18000000e-03,  -3.33000000e-03,\n",
       "        -4.98000000e-03,  -3.13500000e-02,  -1.71000000e-03,\n",
       "        -5.40100000e-02,  -8.38000000e-03,  -1.30924000e+00,\n",
       "        -6.75000000e-03,  -1.66600000e-02,  -9.16000000e-03,\n",
       "        -1.19300000e-02,  -1.95300000e-02,  -1.82700000e-02,\n",
       "        -5.83700000e-02,   0.00000000e+00,  -9.80000000e-04,\n",
       "        -6.16400000e-02,   6.33930000e-01,   3.08658000e+00,\n",
       "        -4.93780000e-01,  -4.94700000e-01,  -2.95180000e-01,\n",
       "        -2.95580000e-01,   5.71720000e-01,  -2.70410000e-01,\n",
       "        -4.42400000e-01,   9.08900000e-01,   8.56020000e-01,\n",
       "         7.61810000e-01,  -3.71170000e-01,   4.18069000e+00,\n",
       "        -3.91840000e-01,  -4.95270000e-01,  -4.93190000e-01,\n",
       "        -3.06120000e-01,  -3.02600000e-01])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.00228180e-06,   3.35547193e-05,   1.59903686e-06,\n",
       "         5.88455350e-06,   4.86902463e-05,   2.98202591e-04,\n",
       "         7.99740592e-05,   8.51102232e-06,   6.04503963e-04,\n",
       "         1.03884468e-05,   3.89677698e-05,   7.87762880e-01,\n",
       "         4.59654511e-05,   5.14870189e-05,   4.54760244e-04,\n",
       "         4.27797379e-04,   2.76705487e-05,   1.80990290e-04,\n",
       "         2.28837901e-03,   4.85179153e-05,   7.37562414e-06,\n",
       "         2.05380256e-05,   8.88946772e-07,   2.17084438e-04,\n",
       "         5.31000410e-07,   7.56725422e-06,   2.21778741e-04,\n",
       "         2.21505252e-06,   6.37791634e-01,   2.51313686e-05,\n",
       "         4.01417338e-07,   5.31951741e-07,   8.93490911e-01,\n",
       "         7.71922886e-01,   1.66875748e-06,   9.99683619e-01,\n",
       "         1.10073231e-01,   1.96198494e-06,   6.72947863e-06,\n",
       "         8.62122700e-03,   4.34382207e-04], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_decode[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('STEP :', '0001', 'cost = ', '1.176950030')\n",
      "('STEP :', '0011', 'cost = ', '2.603309765')\n",
      "('STEP :', '0021', 'cost = ', '0.993005706')\n",
      "('STEP :', '0031', 'cost = ', '1.107182420')\n",
      "('STEP :', '0041', 'cost = ', '2.761034306')\n",
      "('STEP :', '0051', 'cost = ', '0.920144843')\n",
      "('STEP :', '0061', 'cost = ', '0.964722185')\n",
      "('STEP :', '0071', 'cost = ', '2.464248091')\n",
      "('STEP :', '0081', 'cost = ', '0.854233926')\n",
      "('STEP :', '0091', 'cost = ', '0.975328021')\n",
      "('Total cost : ', 111.13502444422929)\n",
      "('Average cost : ', 1.111350244442293)\n"
     ]
    }
   ],
   "source": [
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(testSet[i] - encode_decode[i], 2))\n",
    "    if step % 10 == 0 :\n",
    "        print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/100)\n",
    "print(\"Total cost : \", totalCost)\n",
    "print(\"Average cost : \", averageCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
