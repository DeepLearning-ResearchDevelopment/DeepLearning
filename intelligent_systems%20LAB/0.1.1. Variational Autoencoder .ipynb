{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "usage: __main__.py [-h] [--num_epochs NUM_EPOCHS] [--L L] [--z_dim Z_DIM]\n",
      "                   [--n_hid N_HID] [--binary] [--continuous]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1001/jupyter/kernel-483c3807-b74a-4f70-802d-c27d340dd6ea.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import lasagne as nn\n",
    "import time\n",
    "from PIL import Image\n",
    "from scipy.stats import norm\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "# ############################################################################\n",
    "# Tencia Lee\n",
    "# Some code borrowed from:\n",
    "# https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n",
    "#\n",
    "# Implementation of variational autoencoder (AEVB) algorithm as in:\n",
    "# [1] arXiv:1312.6114 [stat.ML] (Diederik P Kingma, Max Welling 2013)\n",
    "\n",
    "# ################## Download and prepare the MNIST dataset ##################\n",
    "# For the linked MNIST data, the autoencoder learns well only in binary mode.\n",
    "# This is most likely due to the distribution of the values. Most pixels are\n",
    "# either very close to 0, or very close to 1.\n",
    "#\n",
    "# Running this code with default settings should produce a manifold similar\n",
    "# to the example in this directory. An animation of the manifold's evolution\n",
    "# can be found here: https://youtu.be/pgmnCU_DxzM\n",
    "\n",
    "def load_dataset():\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    import gzip\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        data = data.reshape(-1, 1, 28, 28).transpose(0,1,3,2)\n",
    "        return data / np.float32(255)\n",
    "\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "# ############################# Output images ################################\n",
    "# image processing using PIL\n",
    "\n",
    "def get_image_array(X, index, shp=(28,28), channels=1):\n",
    "    ret = (X[index] * 255.).reshape(channels,shp[0],shp[1]) \\\n",
    "            .transpose(2,1,0).clip(0,255).astype(np.uint8)\n",
    "    if channels == 1:\n",
    "        ret = ret.reshape(shp[1], shp[0])\n",
    "    return ret\n",
    "\n",
    "def get_image_pair(X, Xpr, channels=1, idx=-1):\n",
    "    mode = 'RGB' if channels == 3 else 'L'\n",
    "    shp=X[0][0].shape\n",
    "    i = np.random.randint(X.shape[0]) if idx == -1 else idx\n",
    "    orig = Image.fromarray(get_image_array(X, i, shp, channels), mode=mode)\n",
    "    ret = Image.new(mode, (orig.size[0], orig.size[1]*2))\n",
    "    ret.paste(orig, (0,0))\n",
    "    new = Image.fromarray(get_image_array(Xpr, i, shp, channels), mode=mode)\n",
    "    ret.paste(new, (0, orig.size[1]))\n",
    "    return ret\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt]\n",
    "\n",
    "\n",
    "# ##################### Custom layer for middle of VCAE ######################\n",
    "# This layer takes the mu and sigma (both DenseLayers) and combines them with\n",
    "# a random vector epsilon to sample values for a multivariate Gaussian\n",
    "\n",
    "class GaussianSampleLayer(nn.layers.MergeLayer):\n",
    "    def __init__(self, mu, logsigma, rng=None, **kwargs):\n",
    "        self.rng = rng if rng else RandomStreams(nn.random.get_rng().randint(1,2147462579))\n",
    "        super(GaussianSampleLayer, self).__init__([mu, logsigma], **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, logsigma = inputs\n",
    "        shape=(self.input_shapes[0][0] or inputs[0].shape[0],\n",
    "                self.input_shapes[0][1] or inputs[0].shape[1])\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        return mu + tf.exp(logsigma) * self.rng.normal(shape)\n",
    "\n",
    "# ############################## Build Model #################################\n",
    "# encoder has 1 hidden layer, where we get mu and sigma for Z given an inp X\n",
    "# continuous decoder has 1 hidden layer, where we get mu and sigma for X given code Z\n",
    "# binary decoder has 1 hidden layer, where we calculate p(X=1)\n",
    "# once we have (mu, sigma) for Z, we sample L times\n",
    "# Then L separate outputs are constructed and the final layer averages them\n",
    "\n",
    "def build_vae(inputvar, L=2, binary=True, imgshape=(28,28), channels=1, z_dim=2, n_hid=1024):\n",
    "    x_dim = imgshape[0] * imgshape[1] * channels\n",
    "    l_input = nn.layers.InputLayer(shape=(None,channels,imgshape[0], imgshape[1]),\n",
    "            input_var=inputvar, name='input')\n",
    "    l_enc_hid = nn.layers.DenseLayer(l_input, num_units=n_hid,\n",
    "            nonlinearity=nn.nonlinearities.tanh if binary else tf.nnet.softplus,\n",
    "            name='enc_hid')\n",
    "    l_enc_mu = nn.layers.DenseLayer(l_enc_hid, num_units=z_dim,\n",
    "            nonlinearity = None, name='enc_mu')\n",
    "    l_enc_logsigma = nn.layers.DenseLayer(l_enc_hid, num_units=z_dim,\n",
    "            nonlinearity = None, name='enc_logsigma')\n",
    "    l_dec_mu_list = []\n",
    "    l_dec_logsigma_list = []\n",
    "    l_output_list = []\n",
    "    # tie the weights of all L versions so they are the \"same\" layer\n",
    "    W_dec_hid = None\n",
    "    b_dec_hid = None\n",
    "    W_dec_mu = None\n",
    "    b_dec_mu = None\n",
    "    W_dec_ls = None\n",
    "    b_dec_ls = None\n",
    "    for i in xrange(L):\n",
    "        l_Z = GaussianSampleLayer(l_enc_mu, l_enc_logsigma, name='Z')\n",
    "        l_dec_hid = nn.layers.DenseLayer(l_Z, num_units=n_hid,\n",
    "                nonlinearity = nn.nonlinearities.tanh if binary else tf.nnet.softplus,\n",
    "                W=nn.init.GlorotUniform() if W_dec_hid is None else W_dec_hid,\n",
    "                b=nn.init.Constant(0.) if b_dec_hid is None else b_dec_hid,\n",
    "                name='dec_hid')\n",
    "        if binary:\n",
    "            l_output = nn.layers.DenseLayer(l_dec_hid, num_units = x_dim,\n",
    "                    nonlinearity = nn.nonlinearities.sigmoid,\n",
    "                    W = nn.init.GlorotUniform() if W_dec_mu is None else W_dec_mu,\n",
    "                    b = nn.init.Constant(0.) if b_dec_mu is None else b_dec_mu,\n",
    "                    name = 'dec_output')\n",
    "            l_output_list.append(l_output)\n",
    "            if W_dec_hid is None:\n",
    "                W_dec_hid = l_dec_hid.W\n",
    "                b_dec_hid = l_dec_hid.b\n",
    "                W_dec_mu = l_output.W\n",
    "                b_dec_mu = l_output.b\n",
    "        else:\n",
    "            l_dec_mu = nn.layers.DenseLayer(l_dec_hid, num_units=x_dim,\n",
    "                    nonlinearity = None,\n",
    "                    W = nn.init.GlorotUniform() if W_dec_mu is None else W_dec_mu,\n",
    "                    b = nn.init.Constant(0) if b_dec_mu is None else b_dec_mu,\n",
    "                    name = 'dec_mu')\n",
    "            # relu_shift is for numerical stability - if training data has any\n",
    "            # dimensions where stdev=0, allowing logsigma to approach -inf\n",
    "            # will cause the loss function to become NAN. So we set the limit\n",
    "            # stdev >= exp(-1 * relu_shift)\n",
    "            relu_shift = 10\n",
    "            l_dec_logsigma = nn.layers.DenseLayer(l_dec_hid, num_units=x_dim,\n",
    "                    W = nn.init.GlorotUniform() if W_dec_ls is None else W_dec_ls,\n",
    "                    b = nn.init.Constant(0) if b_dec_ls is None else b_dec_ls,\n",
    "                    nonlinearity = lambda a: tf.nnet.relu(a+relu_shift)-relu_shift,\n",
    "                    name='dec_logsigma')\n",
    "            l_output = GaussianSampleLayer(l_dec_mu, l_dec_logsigma,\n",
    "                    name='dec_output')\n",
    "            l_dec_mu_list.append(l_dec_mu)\n",
    "            l_dec_logsigma_list.append(l_dec_logsigma)\n",
    "            l_output_list.append(l_output)\n",
    "            if W_dec_hid is None:\n",
    "                W_dec_hid = l_dec_hid.W\n",
    "                b_dec_hid = l_dec_hid.b\n",
    "                W_dec_mu = l_dec_mu.W\n",
    "                b_dec_mu = l_dec_mu.b\n",
    "                W_dec_ls = l_dec_logsigma.W\n",
    "                b_dec_ls = l_dec_logsigma.b\n",
    "    l_output = nn.layers.ElemwiseSumLayer(l_output_list, coeffs=1./L, name='output')\n",
    "    return l_enc_mu, l_enc_logsigma, l_dec_mu_list, l_dec_logsigma_list, l_output_list, l_output\n",
    "\n",
    "# ############################## Main program ################################\n",
    "\n",
    "def log_likelihood(tgt, mu, ls):\n",
    "    return tf.sum(-(np.float32(0.5 * np.log(2 * np.pi)) + ls)\n",
    "            - 0.5 * tf.sqr(tgt - mu) / tf.exp(2 * ls))\n",
    "\n",
    "def main(L=2, z_dim=2, n_hid=1024, num_epochs=300, binary=True):\n",
    "    print(\"Loading data...\")\n",
    "    X_train, X_val, X_test = load_dataset()\n",
    "    width, height = X_train.shape[2], X_train.shape[3]\n",
    "    input_var = tf.tensor4('inputs')\n",
    "\n",
    "    # Create VAE model\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    print(\"L = {}, z_dim = {}, n_hid = {}, binary={}\".format(L, z_dim, n_hid, binary))\n",
    "    x_dim = width * height\n",
    "    l_z_mu, l_z_ls, l_x_mu_list, l_x_ls_list, l_x_list, l_x = \\\n",
    "           build_vae(input_var, L=L, binary=binary, z_dim=z_dim, n_hid=n_hid)\n",
    "\n",
    "    def build_loss(deterministic):\n",
    "        layer_outputs = nn.layers.get_output([l_z_mu, l_z_ls] + l_x_mu_list + l_x_ls_list\n",
    "                + l_x_list + [l_x], deterministic=deterministic)\n",
    "        z_mu =  layer_outputs[0]\n",
    "        z_ls =  layer_outputs[1]\n",
    "        x_mu =  [] if binary else layer_outputs[2:2+L]\n",
    "        x_ls =  [] if binary else layer_outputs[2+L:2+2*L]\n",
    "        x_list =  layer_outputs[2:2+L] if binary else layer_outputs[2+2*L:2+3*L]\n",
    "        x = layer_outputs[-1]\n",
    "        # Loss expression has two parts as specified in [1]\n",
    "        # kl_div = KL divergence between p_theta(z) and p(z|x)\n",
    "        # - divergence between prior distr and approx posterior of z given x\n",
    "        # - or how likely we are to see this z when accounting for Gaussian prior\n",
    "        # logpxz = log p(x|z)\n",
    "        # - log-likelihood of x given z\n",
    "        # - in binary case logpxz = cross-entropy\n",
    "        # - in continuous case, is log-likelihood of seeing the target x under the\n",
    "        #   Gaussian distribution parameterized by dec_mu, sigma = exp(dec_logsigma)\n",
    "        kl_div = 0.5 * tf.sum(1 + 2*z_ls - tf.sqr(z_mu) - tf.exp(2 * z_ls))\n",
    "        if binary:\n",
    "            logpxz = sum(nn.objectives.binary_crossentropy(x,\n",
    "                input_var.flatten(2)).sum() for x in x_list) * (-1./L)\n",
    "            prediction = x_list[0] if deterministic else x\n",
    "        else:\n",
    "            logpxz = sum(log_likelihood(input_var.flatten(2), mu, ls)\n",
    "                for mu, ls in zip(x_mu, x_ls))/L\n",
    "            prediction = x_mu[0] if deterministic else tf.sum(x_mu, axis=0)/L\n",
    "        loss = -1 * (logpxz + kl_div)\n",
    "        return loss, prediction\n",
    "\n",
    "    # If there are dropout layers etc these functions return masked or non-masked expressions\n",
    "    # depending on if they will be used for training or validation/test err calcs\n",
    "    loss, _ = build_loss(deterministic=False)\n",
    "    test_loss, test_prediction = build_loss(deterministic=True)\n",
    "\n",
    "    # ADAM updates\n",
    "    params = nn.layers.get_all_params(l_x, trainable=True)\n",
    "    updates = nn.updates.adam(loss, params, learning_rate=1e-4)\n",
    "    train_fn = theano.function([input_var], loss, updates=updates)\n",
    "    val_fn = theano.function([input_var], test_loss)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    batch_size = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, batch_size, shuffle=True):\n",
    "            this_err = train_fn(batch)\n",
    "            train_err += this_err\n",
    "            train_batches += 1\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, batch_size, shuffle=False):\n",
    "            err = val_fn(batch)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "\n",
    "    test_err = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test, batch_size, shuffle=False):\n",
    "        err = val_fn(batch)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "    test_err /= test_batches\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err))\n",
    "\n",
    "    # save some example pictures so we can see what it's done \n",
    "    example_batch_size = 20\n",
    "    X_comp = X_test[:example_batch_size]\n",
    "    pred_fn = theano.function([input_var], test_prediction)\n",
    "    X_pred = pred_fn(X_comp).reshape(-1, 1, width, height)\n",
    "    for i in range(20):\n",
    "        get_image_pair(X_comp, X_pred, idx=i, channels=1).save('output_{}.jpg'.format(i))\n",
    "\n",
    "    # save the parameters so they can be loaded for next time\n",
    "    print(\"Saving\")\n",
    "    fn = 'params_{:.6f}'.format(test_err)\n",
    "    np.savez(fn + '.npz', *nn.layers.get_all_param_values(l_x))\n",
    "\n",
    "    # sample from latent space if it's 2d\n",
    "    if z_dim == 2:\n",
    "        # functions for generating images given a code (used for visualization)\n",
    "        # for an given code z, we deterministically take x_mu as the generated data\n",
    "        # (no Gaussian noise is used to either encode or decode).\n",
    "        z_var = tf.vector()\n",
    "        if binary:\n",
    "            generated_x = nn.layers.get_output(l_x, {l_z_mu:z_var}, deterministic=True)\n",
    "        else:\n",
    "            generated_x = nn.layers.get_output(l_x_mu_list[0], {l_z_mu:z_var}, \n",
    "                    deterministic=True)\n",
    "        gen_fn = theano.function([z_var], generated_x)\n",
    "        im = Image.new('L', (width*19,height*19))\n",
    "        for (x,y),val in np.ndenumerate(np.zeros((19,19))):\n",
    "            z = np.asarray([norm.ppf(0.05*(x+1)), norm.ppf(0.05*(y+1))],\n",
    "                    dtype=theano.config.floatX)\n",
    "            x_gen = gen_fn(z).reshape(-1, 1, width, height)\n",
    "            im.paste(Image.fromarray(get_image_array(x_gen,0)), (x*width,y*height))\n",
    "            im.save('gen.jpg')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Arguments - integers, except for binary/continous. Default uses binary.\n",
    "    # Run with option --continuous for continuous output.\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Command line options')\n",
    "    parser.add_argument('--num_epochs', type=int, dest='num_epochs')\n",
    "    parser.add_argument('--L', type=int, dest='L')\n",
    "    parser.add_argument('--z_dim', type=int, dest='z_dim')\n",
    "    parser.add_argument('--n_hid', type=int, dest='n_hid')\n",
    "    parser.add_argument('--binary', dest='binary', action='store_true')\n",
    "    parser.add_argument('--continuous', dest='binary', action='store_false')\n",
    "    parser.set_defaults(binary=True)\n",
    "    args = parser.parse_args(sys.argv[1:])\n",
    "    main(**{k:v for (k,v) in vars(args).items() if v is not None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
