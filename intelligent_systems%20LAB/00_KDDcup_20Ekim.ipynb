{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  KDDcup 99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from array import *\n",
    "from collections import Counter\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam satir sayisi :  1074992\n"
     ]
    }
   ],
   "source": [
    "with open('/home/isl-eyup/tensorflow/KDDcup_datasets/kddcup_data_corrected_uniq.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_as_list = list(reader)\n",
    "def file_len(adress):\n",
    "  f = open(adress)\n",
    "  nr_of_lines = sum(1 for line in f)\n",
    "  f.close()\n",
    "  return nr_of_lines\n",
    "\n",
    "number_of_lines = file_len('/home/isl-eyup/tensorflow/KDDcup_datasets/kddcup_data_corrected_uniq.csv')\n",
    "print('Toplam satir sayisi : ', number_of_lines)\n",
    "\n",
    "\n",
    "\n",
    "def label_index(str):\n",
    "    indx = 0\n",
    "    if str == 'back.':\n",
    "        indx = 0\n",
    "    elif str == 'buffer_overflow.':\n",
    "        indx = 1\n",
    "    elif str =='ftp_write.':\n",
    "        indx = 2\n",
    "    elif str == 'guess_password.':\n",
    "        indx = 3\n",
    "    elif str == 'imap.':\n",
    "        indx = 4\n",
    "    elif str == 'ipsweep.':\n",
    "        indx = 5\n",
    "    elif str == 'land.':\n",
    "        indx = 6\n",
    "    elif str == 'loadmodule.':\n",
    "        indx = 7\n",
    "    elif str == 'multihop.':\n",
    "        indx = 8\n",
    "    elif str == 'neptune.':\n",
    "        indx = 9\n",
    "    elif str == 'nmap.':\n",
    "        indx = 10\n",
    "    elif str == 'normal.':\n",
    "        indx = 11\n",
    "    elif str == 'perl.':\n",
    "        indx = 12\n",
    "    elif str =='phf.':\n",
    "        indx = 13\n",
    "    elif str == 'pod.':\n",
    "        indx = 14\n",
    "    elif str == 'portsweep.':\n",
    "        indx = 15\n",
    "    elif str == 'rootkit.':\n",
    "        indx = 16\n",
    "    elif str == 'satan.':\n",
    "        indx = 17\n",
    "    elif str == 'smurf.':\n",
    "        indx = 18\n",
    "    elif str == 'spy.':\n",
    "        indx = 19\n",
    "    elif str == 'teardrop.':\n",
    "        indx = 20\n",
    "    elif str == 'warezclient.':\n",
    "        indx == 21\n",
    "    else :\n",
    "        indx = 22\n",
    "    return indx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LABEL ARRAY \n",
    "#label_array = np.empty(number_of_lines, dtype=object)\n",
    "#attack_matrix = np.ndarray(shape=(number_of_lines,23), dtype='int')\n",
    "\n",
    "#for i in range(number_of_lines):\n",
    "#    for j in range(23):\n",
    "#        attack_matrix[i][j]=0\n",
    "\n",
    "#FROM LABEL TO ATTACK MATRIX        \n",
    "#for i in range(number_of_lines):\n",
    "#    label_array[i] = data_as_list[i][41] \n",
    "#    attack_matrix[i][label_index(label_array[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 protocol types \n",
      "Protocols : ['icmp', 'tcp', 'udp']\n",
      "There are 70 service \n",
      "Services : ['eco_i', 'ecr_i', 'red_i', 'tim_i', 'urh_i', 'urp_i', 'aol', 'auth', 'bgp', 'courier', 'csnet_ns', 'ctf', 'daytime', 'discard', 'domain', 'echo', 'efs', 'exec', 'finger', 'ftp_data', 'ftp', 'gopher', 'harvest', 'hostnames', 'http_2784', 'http_443', 'http_8001', 'http', 'imap4', 'IRC', 'iso_tsap', 'klogin', 'kshell', 'ldap', 'link', 'login', 'mtp', 'name', 'netbios_dgm', 'netbios_ns', 'netbios_ssn', 'netstat', 'nnsp', 'nntp', 'other', 'pm_dump', 'pop_2', 'pop_3', 'printer', 'private', 'remote_job', 'rje', 'shell', 'smtp', 'sql_net', 'ssh', 'sunrpc', 'supdup', 'systat', 'telnet', 'time', 'uucp_path', 'uucp', 'vmnet', 'whois', 'X11', 'Z39_50', 'domain_u', 'ntp_u', 'tftp_u']\n",
      "There are 11 flag \n",
      "Flags : ['SF', 'REJ', 'RSTO', 'RSTR', 'S0', 'SH', 'S1', 'OTH', 'S2', 'S3', 'RSTOS0']\n"
     ]
    }
   ],
   "source": [
    "protocol = []\n",
    "sayac =0\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][1] in protocol:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        protocol.append(data_as_list[i][1]) \n",
    "        sayac += 1\n",
    "print(\"There are %d protocol types \" %sayac)\n",
    "print(\"Protocols : %s\" %protocol)\n",
    "\n",
    "\n",
    "service = []\n",
    "sayac =0\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][2] in service:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        service.append(data_as_list[i][2])\n",
    "        sayac += 1\n",
    "print(\"There are %d service \" %sayac)   \n",
    "print(\"Services : %s\" %service)\n",
    "\n",
    "\n",
    "\n",
    "flag = []\n",
    "sayac =0\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][3] in flag:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        flag.append(data_as_list[i][3])\n",
    "        sayac += 1\n",
    "print(\"There are %d flag \" %sayac)     \n",
    "print(\"Flags : %s\" %flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Protocols : ['icmp', 'tcp', 'udp']\"\"\"\n",
    "def protocol_index(str):\n",
    "    indx = 0\n",
    "    if str == 'icmp':\n",
    "        indx = 0\n",
    "    elif str == 'tcp':\n",
    "        indx = 1\n",
    "    elif str =='udp':\n",
    "        indx = 2\n",
    "    return indx\n",
    "\n",
    "\"\"\"FLAGS : ['SF', 'REJ', 'RSTO', 'RSTR', 'S0', 'SH', 'S1', 'OTH', 'S2', 'S3', 'RSTOS0']\"\"\"\n",
    "def flag_index(str):\n",
    "    indx = 0\n",
    "    if str == 'SF':\n",
    "        indx = 0\n",
    "    elif str == 'REJ':\n",
    "        indx = 1\n",
    "    elif str =='RSTO':\n",
    "        indx = 2\n",
    "    elif str == 'RSTR':\n",
    "        indx = 3\n",
    "    elif str == 'S0':\n",
    "        indx = 4\n",
    "    elif str == 'SH':\n",
    "        indx = 5\n",
    "    elif str == 'S1':\n",
    "        indx = 6\n",
    "    elif str == 'OTH':\n",
    "        indx = 7\n",
    "    elif str == 'S2':\n",
    "        indx = 8\n",
    "    elif str == 'S3':\n",
    "        indx = 9\n",
    "    elif str == 'nmap.':\n",
    "        indx = 10\n",
    "    elif str == 'RSTOS0':\n",
    "        indx = 11\n",
    "    return indx\n",
    "\n",
    "\n",
    "\"\"\"THERE ARE 70 SERVICES \"\"\"\n",
    "def service_index(str):\n",
    "    indx = 0\n",
    "    if str == 'eco_i':\n",
    "        indx = 0\n",
    "    elif str == 'ecr_i':\n",
    "        indx = 1\n",
    "    elif str =='red_i':\n",
    "        indx = 2\n",
    "    elif str == 'tim_i':\n",
    "        indx = 3\n",
    "    elif str == 'urh_i':\n",
    "        indx = 4\n",
    "    elif str == 'urp_i':\n",
    "        indx = 5\n",
    "    elif str == 'aol':\n",
    "        indx = 6\n",
    "    elif str == 'auth':\n",
    "        indx = 7\n",
    "    elif str == 'bgp':\n",
    "        indx = 8\n",
    "    elif str == 'courier':\n",
    "        indx = 9\n",
    "    elif str == 'csnet_ns':\n",
    "        indx = 10\n",
    "    elif str == 'ctf':\n",
    "        indx = 11\n",
    "    elif str == 'daytime':\n",
    "        indx = 12\n",
    "    elif str == 'discard':\n",
    "        indx = 13\n",
    "    elif str == 'domain':\n",
    "        indx = 14\n",
    "    elif str == 'echo':\n",
    "        indx = 15\n",
    "    elif str == 'efs':\n",
    "        indx = 16\n",
    "    elif str == 'exec':\n",
    "        indx = 17\n",
    "    elif str == 'finger':\n",
    "        indx = 18\n",
    "    elif str == 'ftp_data':\n",
    "        indx = 19\n",
    "    elif str == 'ftp':\n",
    "        indx = 20\n",
    "    elif str == 'gopher':\n",
    "        indx = 21\n",
    "    elif str == 'harvest':\n",
    "        indx = 22\n",
    "    elif str == 'hostnames':\n",
    "        indx = 23\n",
    "    elif str == 'http_2784':\n",
    "        indx = 24\n",
    "    elif str == 'http_443':\n",
    "        indx = 25\n",
    "    elif str == 'http_8001':\n",
    "        indx = 26\n",
    "    elif str == 'http':\n",
    "        indx = 27\n",
    "    elif str == 'imap4':\n",
    "        indx = 28\n",
    "    elif str == 'IRC':\n",
    "        indx = 29\n",
    "    elif str == 'iso_tsap':\n",
    "        indx = 30\n",
    "    elif str == 'klogin':\n",
    "        indx = 31\n",
    "    elif str == 'kshell':\n",
    "        indx = 32        \n",
    "    elif str == 'ldap':\n",
    "        indx = 33\n",
    "    elif str == 'link':\n",
    "        indx = 34\n",
    "    elif str == 'login':\n",
    "        indx = 35\n",
    "    elif str == 'mtp':\n",
    "        indx = 36\n",
    "    elif str == 'name':\n",
    "        indx = 37\n",
    "    elif str == 'netbios_dgm':\n",
    "        indx = 38\n",
    "    elif str == 'netbios_ns':\n",
    "        indx = 39\n",
    "    elif str == 'netbios_ssn':\n",
    "        indx = 40\n",
    "    elif str == 'netstat':\n",
    "        indx = 41\n",
    "    elif str == 'nnsp':\n",
    "        indx = 42\n",
    "    elif str == 'nntp':\n",
    "        indx = 43\n",
    "    elif str == 'other':\n",
    "        indx = 44\n",
    "    elif str == 'pm_dump':\n",
    "        indx = 45\n",
    "    elif str == 'pop_2':\n",
    "        indx = 46\n",
    "    elif str == 'pop_3':\n",
    "        indx = 47\n",
    "    elif str == 'printer':\n",
    "        indx = 48\n",
    "    elif str == 'private':\n",
    "        indx = 49\n",
    "    elif str == 'remote_job':\n",
    "        indx = 50\n",
    "    elif str == 'rje':\n",
    "        indx = 51\n",
    "    elif str == 'shell':\n",
    "        indx = 52\n",
    "    elif str == 'smtp':\n",
    "        indx = 53\n",
    "    elif str == 'sql_net':\n",
    "        indx = 54\n",
    "    elif str == 'ssh':\n",
    "        indx = 55\n",
    "    elif str == 'sunrpc':\n",
    "        indx = 56\n",
    "    elif str == 'supdup':\n",
    "        indx = 57\n",
    "    elif str == 'systat':\n",
    "        indx = 58\n",
    "    elif str == 'telnet':\n",
    "        indx = 59\n",
    "    elif str == 'time':\n",
    "        indx = 60\n",
    "    elif str == 'uucp_path':\n",
    "        indx = 61\n",
    "    elif str == 'uucp':\n",
    "        indx = 62\n",
    "    elif str == 'vmnet':\n",
    "        indx = 63\n",
    "    elif str == 'whois':\n",
    "        indx = 64\n",
    "    elif str == 'X11':\n",
    "        indx = 65\n",
    "    elif str == 'Z39_50':\n",
    "        indx = 66\n",
    "    elif str == 'domain_u':\n",
    "        indx = 67\n",
    "    elif str == 'ntp_u':\n",
    "        indx = 68\n",
    "    elif str == 'tftp_u':\n",
    "        indx = 69\n",
    "    return indx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Array generated \n",
      "Protocol, service and flag names change with int value \n",
      "Dataset reorganize as data_uniq ... \n"
     ]
    }
   ],
   "source": [
    "data_uniq = np.empty(shape=[number_of_lines, 42])\n",
    "label_array = np.empty(number_of_lines, dtype=object)\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    label_array[i] = data_as_list[i][41] \n",
    "print(\"Label Array generated \")\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    data_uniq[i][0] = data_as_list[i][0]\n",
    "    data_uniq[i][1] = protocol_index(data_as_list[i][1])\n",
    "    data_uniq[i][2] = service_index(data_as_list[i][2])\n",
    "    data_uniq[i][3] = flag_index(data_as_list[i][3])\n",
    "print(\"Protocol, service and flag names change with int value \")        \n",
    "        \n",
    "for i in range(number_of_lines):\n",
    "    for j in range(4,41):\n",
    "        data_uniq[i][j] = data_as_list[i][j]\n",
    "\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    data_uniq[i][41] = label_index(label_array[i])\n",
    "\n",
    "print(\"Dataset reorganize as data_uniq ... \")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Array save to csv file \"\"\"\n",
    "np.savetxt(\n",
    "    'deneme1234.csv', # file name\n",
    "    data_uniq,              # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    footer='end of file',   # file footer\n",
    "    comments='# ',          # character to use for comments\n",
    "    header='Data generated by numpy')      # file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shuffled ! \n",
      "(1074992, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/ipykernel/__main__.py:8: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set for class 0: 2266\n",
      "Size of data set for class 1: 32\n",
      "Size of data set for class 2: 10\n",
      "Size of data set for class 3: 0\n",
      "Size of data set for class 4: 3\n",
      "Size of data set for class 5: 20554\n",
      "Size of data set for class 6: 61\n",
      "Size of data set for class 7: 6\n",
      "Size of data set for class 8: 5\n",
      "Size of data set for class 9: 146075\n",
      "Size of data set for class 10: 6047\n",
      "Size of data set for class 11: 880100\n",
      "Size of data set for class 12: 0\n",
      "Size of data set for class 13: 1\n",
      "Size of data set for class 14: 1057\n",
      "Size of data set for class 15: 1334\n",
      "Size of data set for class 16: 4\n",
      "Size of data set for class 17: 1976\n",
      "Size of data set for class 18: 15373\n",
      "Size of data set for class 19: 0\n",
      "Size of data set for class 20: 75\n",
      "Size of data set for class 21: 0\n",
      "Size of data set for class 22: 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "data = data_uniq\n",
    "random.shuffle(data)\n",
    "print(\"Data shuffled ! \")\n",
    "print(data.shape)\n",
    "sayac = np.zeros(23, dtype=int)\n",
    "for i in range(number_of_lines):\n",
    "    sayac[data[i][41]] +=  1\n",
    "    \n",
    "for i in range(23):\n",
    "    print('Size of data set for class ' + str(i) + ': ' + str(sayac[i]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 42)\n"
     ]
    }
   ],
   "source": [
    "#new_data_uniq = np.empty(shape=[300000, 42], dtype = int)\n",
    "#say1 = 0\n",
    "#say2 = 0 \n",
    "\n",
    "#for i in range(number_of_lines):\n",
    "#    for j in range(42):\n",
    "#        if say2 < 300000:\n",
    "#            if data[i][41] == 9 :\n",
    "#                if say1 < 35000:\n",
    "#                    new_data_uniq[say2][j] = data[i][j]\n",
    "#                    say1 += 1\n",
    "#            elif data[i][41] == 11 :\n",
    "#                if say2 < 300000:\n",
    "#                    new_data_uniq[say2][j] = data[i][j]\n",
    "#            else:\n",
    "#                new_data_uniq[say2][j] = data[i][j]\n",
    "#    say2 +=1\n",
    "#print(new_data_uniq.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of insance for class 0: 1110\n",
      "number of insance for class 1: 28\n",
      "number of insance for class 2: 14\n",
      "number of insance for class 3: 0\n",
      "number of insance for class 4: 0\n",
      "number of insance for class 5: 43823\n",
      "number of insance for class 6: 65\n",
      "number of insance for class 7: 0\n",
      "number of insance for class 8: 4\n",
      "number of insance for class 9: 9621\n",
      "number of insance for class 10: 11096\n",
      "number of insance for class 11: 208264\n",
      "number of insance for class 12: 0\n",
      "number of insance for class 13: 0\n",
      "number of insance for class 14: 1502\n",
      "number of insance for class 15: 542\n",
      "number of insance for class 16: 3\n",
      "number of insance for class 17: 646\n",
      "number of insance for class 18: 23276\n",
      "number of insance for class 19: 0\n",
      "number of insance for class 20: 0\n",
      "number of insance for class 21: 0\n",
      "number of insance for class 22: 6\n"
     ]
    }
   ],
   "source": [
    "#sayac = np.zeros(23, dtype=int)\n",
    "#for i in range(300000):\n",
    "#    sayac[new_data_uniq[i][41]] +=  1\n",
    "#    \n",
    "#for i in range(23):\n",
    "#    print('number of insance for class ' + str(i) + ': ' + str(sayac[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ! \n",
      "training set is 80 % of datasets\n",
      "test set is 10 % of datasets\n",
      "valid set is 10 % of datasets\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dataset: new_data_uniq matrix \"\"\"\n",
    "\"\"\"Train_set : 80% of dataset \"\"\"\n",
    "\"\"\"Test set : 10 % of dataset \"\"\"\n",
    "\n",
    "import random\n",
    "data = new_data_uniq\n",
    "random.shuffle(data)\n",
    "dataset = np.empty(shape=[300000, 42], dtype = int)\n",
    "for i in range(300000):\n",
    "    for j in range(42):\n",
    "        dataset[i][j] = int(data[i][j])\n",
    "\n",
    "train_set = dataset[:int(len(dataset)*0.80)]\n",
    "train_labels = np.empty(shape=[240000] ,dtype = int)\n",
    "\n",
    "test_set = np.empty(shape=[30000, 42] ,dtype = int)\n",
    "test_labels = np.empty(shape=[30000] ,dtype = int)\n",
    "\n",
    "valid_set = np.empty(shape=[30000, 42] ,dtype = int)\n",
    "test_labels = np.empty(shape=[30000] ,dtype = int)\n",
    "\n",
    "for i in range(240000):\n",
    "    train_labels[i] = train_set[i][41]\n",
    "\n",
    "for i in range(240000,270000):\n",
    "    test_labels[i] = train_set[i][41]    \n",
    "    \n",
    "for i in range(270000,30000):\n",
    "    valid_labels[i] = valid_set[i][41]\n",
    "\n",
    "k = 0\n",
    "for i in range(240000,270000):\n",
    "    l = 0\n",
    "    for j in range(42):\n",
    "        test_set[k][l] = int(dataset[i][j])\n",
    "        l +=1\n",
    "    k+=1    \n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for i in range(270000,300000):\n",
    "    l = 0\n",
    "    for j in range(42):\n",
    "        valid_set[k][l] = int(dataset[i][j])\n",
    "        l +=1\n",
    "    k +=1\n",
    "        \n",
    "print(\"Done ! \")\n",
    "print(\"training set  \", train_set.shape, train_labels.shape)\n",
    "print(\"test set is \", test_set.shape, test_labels.shape)\n",
    "print(\"valid set is \", valid_set.shape, valid_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (300000, 42)\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 9.09523809524\n",
      "Standard deviation: 39.9611773051\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.11904761905\n",
      "Standard deviation: 15.5444100823\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.5484029555\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.11904761905\n",
      "Standard deviation: 15.5444100823\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.394509726\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 9.09523809524\n",
      "Standard deviation: 39.9611773051\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.5484029555\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.5484029555\n",
      "Mean: 3.11904761905\n",
      "Standard deviation: 15.5444100823\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.394509726\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.11904761905\n",
      "Standard deviation: 15.5444100823\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.394509726\n",
      "Mean: 3.2619047619\n",
      "Standard deviation: 16.444798749\n",
      "Mean: 3.19047619048\n",
      "Standard deviation: 16.148474932\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.394509726\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 3.09523809524\n",
      "Standard deviation: 15.5484029555\n",
      "Mean: 9.09523809524\n",
      "Standard deviation: 39.9611773051\n",
      "Mean: 3.19047619048\n",
      "Standard deviation: 15.9944009478\n",
      "Mean: 3.11904761905\n",
      "Standard deviation: 15.5444100823\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.64285714286\n",
      "Standard deviation: 6.90693725995\n",
      "Mean: 1.0\n",
      "Standard deviation: 3.30223589478\n"
     ]
    }
   ],
   "source": [
    "num_classes = 23\n",
    "array_size = 42 \n",
    "\n",
    "print('Full dataset tensor:', dataset.shape)\n",
    "\n",
    "\"\"\"Mean & Standart deviation of columns \"\"\"\n",
    "for i in range(4,41):\n",
    "    print('Mean:', np.mean(dataset[:][i]))\n",
    "    print('Standard deviation:', np.std(dataset[:][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (240000, 42)\n",
      "Validation: (30000, 42)\n",
      "Testing: (30000, 42)\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "train_size = len(train_set)\n",
    "test_size = len(test_set)\n",
    "valid_size = len(valid_set)\n",
    "\n",
    "\n",
    "print('Training:', train_set.shape)\n",
    "print('Validation:', valid_set.shape)\n",
    "print('Testing:', test_set.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 20 ms, total: 136 ms\n",
      "Wall time: 364 ms\n",
      "CPU times: user 116 ms, sys: 4 ms, total: 120 ms\n",
      "Wall time: 134 ms\n",
      "CPU times: user 644 ms, sys: 32 ms, total: 676 ms\n",
      "Wall time: 1.11 s\n",
      "overlap test valid: 10968\n",
      "overlap train valid: 24398\n",
      "overlap train test: 24054\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#lets use md5 hasing to measure the overlap between something\n",
    "from hashlib import md5\n",
    "#prepare image hashes\n",
    "%time set_valid_set = set([ md5(x).hexdigest() for x in valid_set])\n",
    "%time set_test_set = set([ md5(x).hexdigest() for x in test_set])\n",
    "%time set_train_set = set([ md5(x).hexdigest() for x in train_set])\n",
    "\n",
    "#measure overlaps and print them\n",
    "overlap_test_valid = set_test_set - set_valid_set\n",
    "print('overlap test valid: ' + str(len(overlap_test_valid)))\n",
    "\n",
    "overlap_train_valid = set_train_set - set_valid_set\n",
    "print ('overlap train valid: ' + str(len(overlap_train_valid)))\n",
    "\n",
    "overlap_train_test = set_train_set - set_test_set\n",
    "print ('overlap train test: ' + str(len(overlap_train_test)))\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#en başta tanımladığımız LogisticRegression u kullanacağız\n",
    "logReg = LogisticRegression();\n",
    "\n",
    "fittedmodel = logReg.fit(train_set,train_labels)         \n",
    "test_score = logReg.score(test_set,test_labels)\n",
    "\n",
    "print(test_score)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "num_labels = 23\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_set[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_set)\n",
    "  tf_test_dataset = tf.constant(test_set)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_set)\n",
    "  tf_test_dataset = tf.constant(test_set)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([data_size, 256???]))\n",
    "  biases = tf.Variable(tf.zeros([256???]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([256???, 10???]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  layer_1 = tf.nn.relu(logits)\n",
    "  logits2 = tf.matmul(layer_1, weights2) + biases2     \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits2, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), weights2) + biases2) \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
