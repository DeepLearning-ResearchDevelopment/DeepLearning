{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  KDDcup 99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "from array import *\n",
    "from collections import Counter\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam satir sayisi :  1074992\n",
      "Classes are :  ['dos', 'u2r', 'r2l', 'probe', 'normal']\n"
     ]
    }
   ],
   "source": [
    "with open('/home/isl-eyup/tensorflow/KDDcup_datasets/kddcup_data_corrected_uniq.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_as_list = list(reader)\n",
    "def file_len(adress):\n",
    "  f = open(adress)\n",
    "  nr_of_lines = sum(1 for line in f)\n",
    "  f.close()\n",
    "  return nr_of_lines\n",
    "\n",
    "number_of_lines = file_len('/home/isl-eyup/tensorflow/KDDcup_datasets/kddcup_data_corrected_uniq.csv')\n",
    "print('Toplam satir sayisi : ', number_of_lines)\n",
    "\n",
    "\n",
    "\n",
    "def label_index(str):\n",
    "    indx = 4\n",
    "    if str == 'back.':\n",
    "        indx = 0\n",
    "    elif str == 'buffer_overflow.':\n",
    "        indx = 1\n",
    "    elif str =='ftp_write.':\n",
    "        indx = 2\n",
    "    elif str == 'guess_password.':\n",
    "        indx = 2\n",
    "    elif str == 'imap.':\n",
    "        indx = 2\n",
    "    elif str == 'ipsweep.':\n",
    "        indx = 3\n",
    "    elif str == 'land.':\n",
    "        indx = 0\n",
    "    elif str == 'loadmodule.':\n",
    "        indx = 1\n",
    "    elif str == 'multihop.':\n",
    "        indx = 0\n",
    "    elif str == 'neptune.':\n",
    "        indx = 0\n",
    "    elif str == 'nmap.':\n",
    "        indx = 3\n",
    "    elif str == 'normal.':\n",
    "        indx = 4\n",
    "    elif str == 'perl.':\n",
    "        indx = 1\n",
    "    elif str =='phf.':\n",
    "        indx = 2\n",
    "    elif str == 'pod.':\n",
    "        indx = 0\n",
    "    elif str == 'portsweep.':\n",
    "        indx = 3\n",
    "    elif str == 'rootkit.':\n",
    "        indx = 1\n",
    "    elif str == 'satan.':\n",
    "        indx = 3\n",
    "    elif str == 'smurf.':\n",
    "        indx = 0\n",
    "    elif str == 'spy.':\n",
    "        indx = 2\n",
    "    elif str == 'teardrop.':\n",
    "        indx = 0\n",
    "    elif str == 'warezclient.':\n",
    "        indx == 2\n",
    "    elif str == 'warezmaster.':\n",
    "        indx = 2\n",
    "    else :\n",
    "        indx = 4\n",
    "    return indx\n",
    "    \n",
    "    \n",
    "attack_types = ['dos','u2r','r2l','probe','normal']    \n",
    "print(\"Classes are : \",attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LABEL ARRAY \n",
    "#label_array = np.empty(number_of_lines, dtype=object)\n",
    "#attack_matrix = np.ndarray(shape=(number_of_lines,5), dtype='int')\n",
    "\n",
    "#for i in range(number_of_lines):\n",
    "#    for j in range(5):\n",
    "#        attack_matrix[i][j]=0\n",
    "\n",
    "#FROM LABEL TO ATTACK MATRIX        \n",
    "#for i in range(number_of_lines):\n",
    "#    label_array[i] = data_as_list[i][41] \n",
    "#    attack_matrix[i][label_index(label_array[i])] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Protocols : ['icmp', 'tcp', 'udp']\"\"\"\n",
    "def protocol_index(str):\n",
    "    indx = 0\n",
    "    if str == 'icmp':\n",
    "        indx = 0\n",
    "    elif str == 'tcp':\n",
    "        indx = 1\n",
    "    elif str =='udp':\n",
    "        indx = 2\n",
    "    return indx\n",
    "\n",
    "\"\"\"FLAGS : ['SF', 'REJ', 'RSTO', 'RSTR', 'S0', 'SH', 'S1', 'OTH', 'S2', 'S3', 'RSTOS0']\"\"\"\n",
    "def flag_index(str):\n",
    "    indx = 0\n",
    "    if str == 'SF':\n",
    "        indx = 0\n",
    "    elif str == 'REJ':\n",
    "        indx = 1\n",
    "    elif str =='RSTO':\n",
    "        indx = 2\n",
    "    elif str == 'RSTR':\n",
    "        indx = 3\n",
    "    elif str == 'S0':\n",
    "        indx = 4\n",
    "    elif str == 'SH':\n",
    "        indx = 5\n",
    "    elif str == 'S1':\n",
    "        indx = 6\n",
    "    elif str == 'OTH':\n",
    "        indx = 7\n",
    "    elif str == 'S2':\n",
    "        indx = 8\n",
    "    elif str == 'S3':\n",
    "        indx = 9\n",
    "    elif str == 'nmap.':\n",
    "        indx = 10\n",
    "    elif str == 'RSTOS0':\n",
    "        indx = 11\n",
    "    return indx\n",
    "\n",
    "\n",
    "\"\"\"THERE ARE 70 SERVICES \"\"\"\n",
    "def service_index(str):\n",
    "    indx = 0\n",
    "    if str == 'eco_i':\n",
    "        indx = 0\n",
    "    elif str == 'ecr_i':\n",
    "        indx = 1\n",
    "    elif str =='red_i':\n",
    "        indx = 2\n",
    "    elif str == 'tim_i':\n",
    "        indx = 3\n",
    "    elif str == 'urh_i':\n",
    "        indx = 4\n",
    "    elif str == 'urp_i':\n",
    "        indx = 5\n",
    "    elif str == 'aol':\n",
    "        indx = 6\n",
    "    elif str == 'auth':\n",
    "        indx = 7\n",
    "    elif str == 'bgp':\n",
    "        indx = 8\n",
    "    elif str == 'courier':\n",
    "        indx = 9\n",
    "    elif str == 'csnet_ns':\n",
    "        indx = 10\n",
    "    elif str == 'ctf':\n",
    "        indx = 11\n",
    "    elif str == 'daytime':\n",
    "        indx = 12\n",
    "    elif str == 'discard':\n",
    "        indx = 13\n",
    "    elif str == 'domain':\n",
    "        indx = 14\n",
    "    elif str == 'echo':\n",
    "        indx = 15\n",
    "    elif str == 'efs':\n",
    "        indx = 16\n",
    "    elif str == 'exec':\n",
    "        indx = 17\n",
    "    elif str == 'finger':\n",
    "        indx = 18\n",
    "    elif str == 'ftp_data':\n",
    "        indx = 19\n",
    "    elif str == 'ftp':\n",
    "        indx = 20\n",
    "    elif str == 'gopher':\n",
    "        indx = 21\n",
    "    elif str == 'harvest':\n",
    "        indx = 22\n",
    "    elif str == 'hostnames':\n",
    "        indx = 23\n",
    "    elif str == 'http_2784':\n",
    "        indx = 24\n",
    "    elif str == 'http_443':\n",
    "        indx = 25\n",
    "    elif str == 'http_8001':\n",
    "        indx = 26\n",
    "    elif str == 'http':\n",
    "        indx = 27\n",
    "    elif str == 'imap4':\n",
    "        indx = 28\n",
    "    elif str == 'IRC':\n",
    "        indx = 29\n",
    "    elif str == 'iso_tsap':\n",
    "        indx = 30\n",
    "    elif str == 'klogin':\n",
    "        indx = 31\n",
    "    elif str == 'kshell':\n",
    "        indx = 32        \n",
    "    elif str == 'ldap':\n",
    "        indx = 33\n",
    "    elif str == 'link':\n",
    "        indx = 34\n",
    "    elif str == 'login':\n",
    "        indx = 35\n",
    "    elif str == 'mtp':\n",
    "        indx = 36\n",
    "    elif str == 'name':\n",
    "        indx = 37\n",
    "    elif str == 'netbios_dgm':\n",
    "        indx = 38\n",
    "    elif str == 'netbios_ns':\n",
    "        indx = 39\n",
    "    elif str == 'netbios_ssn':\n",
    "        indx = 40\n",
    "    elif str == 'netstat':\n",
    "        indx = 41\n",
    "    elif str == 'nnsp':\n",
    "        indx = 42\n",
    "    elif str == 'nntp':\n",
    "        indx = 43\n",
    "    elif str == 'other':\n",
    "        indx = 44\n",
    "    elif str == 'pm_dump':\n",
    "        indx = 45\n",
    "    elif str == 'pop_2':\n",
    "        indx = 46\n",
    "    elif str == 'pop_3':\n",
    "        indx = 47\n",
    "    elif str == 'printer':\n",
    "        indx = 48\n",
    "    elif str == 'private':\n",
    "        indx = 49\n",
    "    elif str == 'remote_job':\n",
    "        indx = 50\n",
    "    elif str == 'rje':\n",
    "        indx = 51\n",
    "    elif str == 'shell':\n",
    "        indx = 52\n",
    "    elif str == 'smtp':\n",
    "        indx = 53\n",
    "    elif str == 'sql_net':\n",
    "        indx = 54\n",
    "    elif str == 'ssh':\n",
    "        indx = 55\n",
    "    elif str == 'sunrpc':\n",
    "        indx = 56\n",
    "    elif str == 'supdup':\n",
    "        indx = 57\n",
    "    elif str == 'systat':\n",
    "        indx = 58\n",
    "    elif str == 'telnet':\n",
    "        indx = 59\n",
    "    elif str == 'time':\n",
    "        indx = 60\n",
    "    elif str == 'uucp_path':\n",
    "        indx = 61\n",
    "    elif str == 'uucp':\n",
    "        indx = 62\n",
    "    elif str == 'vmnet':\n",
    "        indx = 63\n",
    "    elif str == 'whois':\n",
    "        indx = 64\n",
    "    elif str == 'X11':\n",
    "        indx = 65\n",
    "    elif str == 'Z39_50':\n",
    "        indx = 66\n",
    "    elif str == 'domain_u':\n",
    "        indx = 67\n",
    "    elif str == 'ntp_u':\n",
    "        indx = 68\n",
    "    elif str == 'tftp_u':\n",
    "        indx = 69\n",
    "    return indx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 protocol types \n",
      "Protocols : ['icmp', 'tcp', 'udp']\n",
      "There are 70 service \n",
      "Services : ['eco_i', 'ecr_i', 'red_i', 'tim_i', 'urh_i', 'urp_i', 'aol', 'auth', 'bgp', 'courier', 'csnet_ns', 'ctf', 'daytime', 'discard', 'domain', 'echo', 'efs', 'exec', 'finger', 'ftp_data', 'ftp', 'gopher', 'harvest', 'hostnames', 'http_2784', 'http_443', 'http_8001', 'http', 'imap4', 'IRC', 'iso_tsap', 'klogin', 'kshell', 'ldap', 'link', 'login', 'mtp', 'name', 'netbios_dgm', 'netbios_ns', 'netbios_ssn', 'netstat', 'nnsp', 'nntp', 'other', 'pm_dump', 'pop_2', 'pop_3', 'printer', 'private', 'remote_job', 'rje', 'shell', 'smtp', 'sql_net', 'ssh', 'sunrpc', 'supdup', 'systat', 'telnet', 'time', 'uucp_path', 'uucp', 'vmnet', 'whois', 'X11', 'Z39_50', 'domain_u', 'ntp_u', 'tftp_u']\n",
      "There are 11 flag \n",
      "Flags : ['SF', 'REJ', 'RSTO', 'RSTR', 'S0', 'SH', 'S1', 'OTH', 'S2', 'S3', 'RSTOS0']\n"
     ]
    }
   ],
   "source": [
    "protocol = []\n",
    "sayac =0\n",
    "\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][1] in protocol:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        protocol.append(data_as_list[i][1]) \n",
    "        sayac += 1\n",
    "print(\"There are %d protocol types \" %sayac)\n",
    "print(\"Protocols : %s\" %protocol)\n",
    "\n",
    "\n",
    "service = []\n",
    "sayac =0\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][2] in service:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        service.append(data_as_list[i][2])\n",
    "        sayac += 1\n",
    "print(\"There are %d service \" %sayac)   \n",
    "print(\"Services : %s\" %service)\n",
    "\n",
    "\n",
    "\n",
    "flag = []\n",
    "sayac =0\n",
    "for i in range(number_of_lines):\n",
    "    if data_as_list[i][3] in flag:\n",
    "        sayac = sayac\n",
    "    else:\n",
    "        flag.append(data_as_list[i][3])\n",
    "        sayac += 1\n",
    "print(\"There are %d flag \" %sayac)     \n",
    "print(\"Flags : %s\" %flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probe'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_types[label_index(data_as_list[1][41])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Array generated \n",
      "Protocol, service and flag names change with int value \n",
      "Dataset reorganize as data_uniq ... \n"
     ]
    }
   ],
   "source": [
    "data_uniq = np.empty(shape=[number_of_lines, 42])\n",
    "label_array = np.empty(number_of_lines, dtype=object)\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    label_array[i] = attack_types[label_index(data_as_list[i][41])]\n",
    "print(\"Label Array generated \")\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    data_uniq[i][0] = data_as_list[i][0]\n",
    "    data_uniq[i][1] = protocol_index(data_as_list[i][1])\n",
    "    data_uniq[i][2] = service_index(data_as_list[i][2])\n",
    "    data_uniq[i][3] = flag_index(data_as_list[i][3])\n",
    "print(\"Protocol, service and flag names change with int value \")        \n",
    "        \n",
    "for i in range(number_of_lines):\n",
    "    for j in range(4,41):\n",
    "        data_uniq[i][j] = data_as_list[i][j]\n",
    "\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    data_uniq[i][41] = attack_types.index(label_array[i])\n",
    "\n",
    "print(\"Dataset reorganize as data_uniq ... \")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\"\"\"Array save to csv file \"\"\"\n",
    "#np.savetxt(\n",
    "#    'Dataset_uniq_corrected.csv', # file name\n",
    "#    data_uniq,              # array to save\n",
    "#    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "#    delimiter=',',          # column delimiter\n",
    "#    newline='\\n',           # new line character\n",
    "#    footer='end of file',   # file footer\n",
    "#    comments='# ',          # character to use for comments\n",
    "#    header='Data generated by numpy')      # file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shuffled ! \n",
      "(300000, 42) float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = np.empty(shape=[300000, 42], dtype = float)\n",
    "data = data_uniq[:300000][:].astype(float)\n",
    "\n",
    "data = shuffle(data)\n",
    "print(\"Data shuffled ! \")\n",
    "print(data.shape, data.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set for class 0: 23501\n",
      "Size of data set for class 1: 12\n",
      "Size of data set for class 2: 7\n",
      "Size of data set for class 3: 4547\n",
      "Size of data set for class 4: 271933\n"
     ]
    }
   ],
   "source": [
    "sayac = np.zeros(5, dtype=int)\n",
    "for i in range(300000):\n",
    "    sayac[data[i][41]] +=  1\n",
    "    \n",
    "for i in range(5):\n",
    "    print('Size of data set for class ' + str(i) + ': ' + str(sayac[i]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 42)\n"
     ]
    }
   ],
   "source": [
    "#new_data_uniq = np.empty(shape=[300000, 42], dtype = int)\n",
    "#say1 = 0\n",
    "#say2 = 0 \n",
    "\n",
    "#for i in range(number_of_lines):\n",
    "#    for j in range(42):\n",
    "#        if say2 < 300000:\n",
    "#            if data[i][41] == 9 :\n",
    "#                if say1 < 35000:\n",
    "#                    new_data_uniq[say2][j] = data[i][j]\n",
    "#                    say1 += 1\n",
    "#            elif data[i][41] == 11 :\n",
    "#                if say2 < 300000:\n",
    "#                    new_data_uniq[say2][j] = data[i][j]\n",
    "#            else:\n",
    "#                new_data_uniq[say2][j] = data[i][j]\n",
    "#    say2 +=1\n",
    "#print(new_data_uniq.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of insance for class 0: 1110\n",
      "number of insance for class 1: 28\n",
      "number of insance for class 2: 14\n",
      "number of insance for class 3: 0\n",
      "number of insance for class 4: 0\n",
      "number of insance for class 5: 43823\n",
      "number of insance for class 6: 65\n",
      "number of insance for class 7: 0\n",
      "number of insance for class 8: 4\n",
      "number of insance for class 9: 9621\n",
      "number of insance for class 10: 11096\n",
      "number of insance for class 11: 208264\n",
      "number of insance for class 12: 0\n",
      "number of insance for class 13: 0\n",
      "number of insance for class 14: 1502\n",
      "number of insance for class 15: 542\n",
      "number of insance for class 16: 3\n",
      "number of insance for class 17: 646\n",
      "number of insance for class 18: 23276\n",
      "number of insance for class 19: 0\n",
      "number of insance for class 20: 0\n",
      "number of insance for class 21: 0\n",
      "number of insance for class 22: 6\n"
     ]
    }
   ],
   "source": [
    "#sayac = np.zeros(5, dtype=int)\n",
    "#for i in range(300000):\n",
    "#    sayac[new_data_uniq[i][41]] +=  1\n",
    "#    \n",
    "#for i in range(5):\n",
    "#    print('number of insance for class ' + str(i) + ': ' + str(sayac[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ! \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dataset: data_uniq matrix --> data matrix \"\"\"\n",
    "\"\"\"Train_set : 80% of dataset \"\"\"\n",
    "\"\"\"Test set : 10 % of dataset \"\"\"\n",
    "\n",
    "dataset = np.empty(shape=[300000, 42], dtype = float)\n",
    "for i in range(300000):\n",
    "    for j in range(42):\n",
    "        dataset[i][j] = int(data[i][j])\n",
    "\n",
    "dataset = np.float32(dataset)  \n",
    "\n",
    "train_dataset = np.empty(shape=[240000, 42] ,dtype = float)\n",
    "train_dataset = np.float32(train_dataset)\n",
    "train_labels = np.empty(shape=[240000] ,dtype = float)\n",
    "train_labels = np.float32(train_labels)\n",
    "test_dataset = np.empty(shape=[30000, 42] ,dtype = float)\n",
    "test_dataset = np.float32(test_dataset)\n",
    "test_labels = np.empty(shape=[30000] ,dtype = float)\n",
    "test_labels = np.float32(test_labels)\n",
    "valid_dataset = np.empty(shape=[30000, 42] ,dtype = float)\n",
    "valid_dataset = np.float32(valid_dataset)\n",
    "valid_labels = np.empty(shape=[30000] ,dtype = float)\n",
    "valid_labels = np.float32(valid_labels)\n",
    "print(\"Done ! \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset created . .  (240000, 42) float32\n",
      "Test dataset created . .  (30000, 42) float32\n",
      "Validation dataset created . .  (30000, 42) float32\n",
      "Done ! \n"
     ]
    }
   ],
   "source": [
    "for i in range(240000):\n",
    "    for j in range(42):\n",
    "        train_dataset[i][j] = int(dataset[i][j])\n",
    "print(\"Train dataset created . . \", train_dataset.shape, train_dataset.dtype)\n",
    "        \n",
    "k = 0\n",
    "for i in range(240000,270000):\n",
    "    l = 0\n",
    "    for j in range(42):\n",
    "        test_dataset[k][l] = int(dataset[i][j])\n",
    "        l +=1\n",
    "    k+=1    \n",
    "print(\"Test dataset created . . \", test_dataset.shape, test_dataset.dtype)\n",
    "    \n",
    "k = 0\n",
    "for i in range(270000,300000):\n",
    "    l = 0\n",
    "    for j in range(42):\n",
    "        valid_dataset[k][l] = int(dataset[i][j])\n",
    "        l +=1\n",
    "    k +=1\n",
    "print(\"Validation dataset created . . \", valid_dataset.shape, valid_dataset.dtype)\n",
    "   \n",
    "print(\"Done ! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels array created :    (240000,) float32\n",
      "test labels array created :   (30000,) float32\n",
      "validation labels array created :   (30000,) float32\n",
      "Done ! \n"
     ]
    }
   ],
   "source": [
    "for i in range(240000):\n",
    "    train_labels[i] = dataset[i][41]\n",
    "print(\"training labels array created :   \", train_labels.shape, train_labels.dtype) \n",
    "\n",
    "k=0    \n",
    "for i in range(240000,270000):\n",
    "    test_labels[k] = dataset[i][41]    \n",
    "    k += 1\n",
    "print(\"test labels array created :  \", test_labels.shape, test_labels.dtype)\n",
    "\n",
    "m=0    \n",
    "for i in range(270000,300000):\n",
    "    valid_labels[m] = dataset[i][41]\n",
    "    m += 1\n",
    "print(\"validation labels array created :  \", valid_labels.shape, valid_labels.dtype)\n",
    "\n",
    "print(\"Done ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of insance for class 0: 23501\n",
      "number of insance for class 1: 12\n",
      "number of insance for class 2: 7\n",
      "number of insance for class 3: 4547\n",
      "number of insance for class 4: 271933\n"
     ]
    }
   ],
   "source": [
    "sayac = np.zeros(5, dtype=int)\n",
    "for i in range(300000):\n",
    "    sayac[dataset[i][41]] +=  1\n",
    "    \n",
    "for i in range(5):\n",
    "    print('number of insance for class ' + str(i) + ': ' + str(sayac[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset tensor: (300000, 42)\n",
      "Means of each columns  : \n",
      "[  0.00000000e+00   9.60309982e-01   2.39767303e+01   3.80233347e-01\n",
      "   1.10602747e+03   3.11069556e+03   8.33333324e-05   6.76666677e-04\n",
      "   0.00000000e+00   1.05799995e-02   1.33333333e-05   7.26830006e-01\n",
      "   6.46666682e-04   8.33333324e-05   0.00000000e+00   8.00933316e-02\n",
      "   2.97666667e-03   1.10999995e-03   9.80000012e-04   0.00000000e+00\n",
      "   0.00000000e+00   2.93333345e-04   2.33072243e+01   1.38064270e+01\n",
      "   5.57533316e-02   5.55966683e-02   1.31770000e-01   1.28623337e-01\n",
      "   9.17853355e-01   5.24333352e-03   5.63500002e-02   1.17942123e+02\n",
      "   1.92000473e+02   7.56556690e-01   6.93333335e-04   5.21466658e-02\n",
      "   1.21999998e-03   5.53233325e-02   5.51900007e-02   1.12669997e-01\n",
      "   6.75633326e-02   3.67132998e+00]\n",
      "Standart deviations of each columns : \n",
      "[  0.00000000e+00   1.95346206e-01   6.23409891e+00   1.00121713e+00\n",
      "   1.48577246e+04   8.53244629e+03   9.12795588e-03   2.59956066e-02\n",
      "   0.00000000e+00   4.49686199e-01   7.30295060e-03   4.45104778e-01\n",
      "   5.24644889e-02   9.12797451e-03   0.00000000e+00   7.50770748e-01\n",
      "   6.44058660e-02   3.32822837e-02   3.12765837e-02   0.00000000e+00\n",
      "   0.00000000e+00   1.71219762e-02   6.41213531e+01   4.14709244e+01\n",
      "   2.29183152e-01   2.28901580e-01   3.38381976e-01   3.34260583e-01\n",
      "   2.74259806e-01   7.20984265e-02   2.30490506e-01   1.02584076e+02\n",
      "   9.56140289e+01   4.28968519e-01   2.63140853e-02   2.22636178e-01\n",
      "   3.48880701e-02   2.28452832e-01   2.28134722e-01   3.15542012e-01\n",
      "   2.51009166e-01   1.07632470e+00]\n"
     ]
    }
   ],
   "source": [
    "print('Full dataset tensor:', dataset.shape)\n",
    "\n",
    "\"\"\"Mean & Standart deviation of columns \"\"\"\n",
    "print(\"Means of each columns  : \")\n",
    "print(dataset.mean(0))\n",
    "print(\"Standart deviations of each columns : \")\n",
    "print(dataset.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (240000, 42)\n",
      "Validation: (30000, 42)\n",
      "Testing: (30000, 42)\n"
     ]
    }
   ],
   "source": [
    "          \n",
    "train_size = len(train_dataset)\n",
    "test_size = len(test_dataset)\n",
    "valid_size = len(valid_dataset)\n",
    "\n",
    "\n",
    "print('Training:', train_dataset.shape)\n",
    "print('Validation:', valid_dataset.shape)\n",
    "print('Testing:', test_dataset.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 8 ms, total: 104 ms\n",
      "Wall time: 96.4 ms\n",
      "CPU times: user 100 ms, sys: 12 ms, total: 112 ms\n",
      "Wall time: 102 ms\n",
      "CPU times: user 516 ms, sys: 32 ms, total: 548 ms\n",
      "Wall time: 509 ms\n",
      "overlap test valid: 28197\n",
      "overlap train valid: 218207\n",
      "overlap train test: 218174\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#lets use md5 hasing to measure the overlap between something\n",
    "from hashlib import md5\n",
    "#prepare image hashes\n",
    "%time set_valid_dataset = set([ md5(x).hexdigest() for x in valid_dataset])\n",
    "%time set_test_dataset = set([ md5(x).hexdigest() for x in test_dataset])\n",
    "%time set_train_dataset = set([ md5(x).hexdigest() for x in train_dataset])\n",
    "\n",
    "#measure overlaps and print them\n",
    "overlap_test_valid = set_test_dataset - set_valid_dataset\n",
    "print('overlap test valid: ' + str(len(overlap_test_valid)))\n",
    "\n",
    "overlap_train_valid = set_train_dataset - set_valid_dataset\n",
    "print ('overlap train valid: ' + str(len(overlap_train_valid)))\n",
    "\n",
    "overlap_train_test = set_train_dataset - set_test_dataset\n",
    "print ('overlap train test: ' + str(len(overlap_train_test)))\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9988\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "#en başta tanımladığımız LogisticRegression u kullanacağız\n",
    "logReg = LogisticRegression();\n",
    "\n",
    "fittedmodel = logReg.fit(train_dataset,train_labels)         \n",
    "test_score = logReg.score(test_dataset,test_labels)\n",
    "\n",
    "print(test_score)\n",
    "print(\"Done !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   1.,  27., ...,   1.,   1.,   4.],\n",
       "       [  0.,   1.,  27., ...,   0.,   0.,   4.],\n",
       "       [  0.,   1.,  27., ...,   0.,   0.,   4.],\n",
       "       ..., \n",
       "       [  0.,   1.,  27., ...,   0.,   0.,   4.],\n",
       "       [  0.,   1.,  27., ...,   0.,   0.,   4.],\n",
       "       [  0.,   1.,  27., ...,   0.,   0.,   4.]])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32_ref"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (240000, 42) (240000, 5)\n",
      "Validation set (30000, 42) (30000, 5)\n",
      "Test set (30000, 42) (30000, 5)\n"
     ]
    }
   ],
   "source": [
    "data_size = 42\n",
    "num_labels = 5\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, data_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_set, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_set, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_set, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'Const:0' shape=(10000, 42) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Const_1:0' shape=(10000,) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Const_2:0' shape=(30000, 42) dtype=float32>>\n",
      "Tensor(\"Placeholder_2:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "num_labels = 5\n",
    "data_size = 42 \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_set[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_set)\n",
    "  tf_test_dataset = tf.constant(test_set)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>: 0.001}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 139.291138\n",
      "Training accuracy: 76.3%\n",
      "Validation accuracy: 20.6%\n",
      "Loss at step 100: 6456.086914\n",
      "Training accuracy: 94.8%\n",
      "Validation accuracy: 97.9%\n",
      "Loss at step 200: 6601.367188\n",
      "Training accuracy: 97.7%\n",
      "Validation accuracy: 97.6%\n",
      "Loss at step 300: 9438.658203\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 97.8%\n",
      "Loss at step 400: 4259.704102\n",
      "Training accuracy: 96.4%\n",
      "Validation accuracy: 97.3%\n",
      "Loss at step 500: 3655.029785\n",
      "Training accuracy: 98.1%\n",
      "Validation accuracy: 97.3%\n",
      "Loss at step 600: 11441.554688\n",
      "Training accuracy: 97.8%\n",
      "Validation accuracy: 97.9%\n",
      "Loss at step 700: 4389.828125\n",
      "Training accuracy: 98.1%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 800: 4143.619141\n",
      "Training accuracy: 98.1%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 900: 1772.096558\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 1000: 2488.612061\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 97.9%\n",
      "Loss at step 1100: 1199.324341\n",
      "Training accuracy: 97.1%\n",
      "Validation accuracy: 96.7%\n",
      "Loss at step 1200: 7147.974609\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.3%\n",
      "Loss at step 1300: 2098.801758\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 1400: 668.752014\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 94.5%\n",
      "Loss at step 1500: 10298.891602\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 1600: 2000.183350\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.3%\n",
      "Loss at step 1700: 2192.050537\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.2%\n",
      "Loss at step 1800: 7467.060547\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 98.4%\n",
      "Loss at step 1900: 6146.123047\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 98.5%\n",
      "Loss at step 2000: 4220.504883\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 98.6%\n",
      "Loss at step 2100: 11289.064453\n",
      "Training accuracy: 97.9%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 2200: 2859.522217\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.1%\n",
      "Loss at step 2300: 3566.947754\n",
      "Training accuracy: 98.3%\n",
      "Validation accuracy: 98.3%\n",
      "Loss at step 2400: 1992.429932\n",
      "Training accuracy: 98.2%\n",
      "Validation accuracy: 98.1%\n",
      "Loss at step 2500: 881.303040\n",
      "Training accuracy: 93.9%\n",
      "Validation accuracy: 98.0%\n",
      "Loss at step 2600: 1311.409912\n",
      "Training accuracy: 98.0%\n",
      "Validation accuracy: 97.9%\n",
      "Loss at step 2700: 5800.579102\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 98.6%\n",
      "Loss at step 2800: 633.481262\n",
      "Training accuracy: 97.5%\n",
      "Validation accuracy: 97.3%\n",
      "Loss at step 2900: 7327.395020\n",
      "Training accuracy: 98.5%\n",
      "Validation accuracy: 98.5%\n",
      "Loss at step 3000: 563.240662\n",
      "Training accuracy: 97.2%\n",
      "Validation accuracy: 96.7%\n",
      "Test accuracy: 96.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    feed_dict = {beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction],feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "# Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    " # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2) \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_hidden_nodes]))\n",
    "  biases = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases), weights2) + biases2) \n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases), weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_regul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'Placeholder_2:0' shape=<unknown> dtype=float32>: 0.001}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 45124.859375\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 500: 196956993279641305177128960.000000\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 96.8%\n",
      "Minibatch loss at step 1000: 119445471782368550565445632.000000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 1500: 72438385564155503846621184.000000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2000: 43930653533750228385005568.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2500: 26641979912087097923600384.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 3000: 16157151493103291172126720.000000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000\n",
      "(240000, 42)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape[0])\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 12873.699219\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 500: 152816671558467584.000000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 1000: 92676417766031360.000000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 1500: 56204070884474880.000000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 2000: 34085214795857920.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 2500: 20671134282285056.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 3000: 12536116183826432.000000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_set[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAF4CAYAAAA1w9ECAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJFKVEQERGRov4/B5v13Cmbk5oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0434bbd110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 5153.262695\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2: 29460582400.000000\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4: 1280205465321472.000000\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 6: 1456864231424.000000\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 8: 32282956005376.000000\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 10: 1.261630\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 12: 0.345780\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 14: 0.340325\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 16: 0.326473\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 18: 0.318057\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 20: 0.314502\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 22: 0.303313\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 24: 0.296748\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 26: 0.294452\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 28: 0.285040\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 30: 0.279762\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 32: 0.278393\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 34: 0.270249\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 36: 0.265933\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 38: 0.265304\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 40: 0.258112\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 42: 0.254553\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 44: 0.254539\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 46: 0.248086\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 48: 0.245140\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 50: 0.245643\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 52: 0.239774\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 54: 0.237327\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 56: 0.238264\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 58: 0.232860\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 60: 0.230820\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 62: 0.232119\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 64: 0.227086\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 66: 0.225377\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 68: 0.226972\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 70: 0.222237\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 72: 0.220796\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 74: 0.222632\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 76: 0.218138\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 78: 0.216913\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 80: 0.218943\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 82: 0.214646\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 84: 0.213597\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 86: 0.215783\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 88: 0.211647\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 90: 0.210741\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 92: 0.213052\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 94: 0.209051\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 96: 0.208262\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 98: 0.210673\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 100: 0.206786\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([data_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 136555.281250\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2: 144381100032.000000\n",
      "Minibatch accuracy: 25.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4: 971695816966144.000000\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 6: 25999974203152488464384.000000\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8: 14088449449407665754079232.000000\n",
      "Minibatch accuracy: 24.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 10: 2217508635394423213778796544.000000\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 12: 849609003565647774404407141072896.000000\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 51.1%\n",
      "Minibatch loss at step 14: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 16: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 18: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 20: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 22: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 24: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 26: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 28: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 30: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 32: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 34: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 36: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 38: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 40: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 42: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 44: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 46: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 48: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 50: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 52: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 54: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 56: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 58: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 60: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 62: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 64: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 66: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 68: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 70: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 72: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 74: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 76: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 78: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 80: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 82: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 84: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 86: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 88: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 90: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 92: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 94: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 96: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 98: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 100: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Test accuracy: 8.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ry to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is 97.1%.\n",
    "One avenue you can explore is to add multiple layers.\n",
    "Another one is to use learning rate decay:\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [data_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (data_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 35.314209\n",
      "Minibatch accuracy: 32.0%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 500: 6956094823819247172343300096.000000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 1000: 4218555501982403885721976832.000000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 1500: 3047860485673561461901754368.000000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2000: 2202039343388162973470031872.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 2500: 1782668850007161181885693952.000000\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 3000: 1443166412785360392712880128.000000\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 3500: 1257992683111170685240606720.000000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4000: 1096579245247634418308218880.000000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 4500: 1002943055820650670437957632.000000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 5000: 917301832849165231914483712.000000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 5500: 865597527671941024528203776.000000\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 6000: 816807955632315515973664768.000000\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 6500: 786578980970822494387699712.000000\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 7000: 757469797461579937423032320.000000\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 7500: 739128937698853478633308160.000000\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8000: 721232275533421945846497280.000000\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8500: 709833515861442750035525632.000000\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 9000: 698613246885120767845990400.000000\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.8%\n",
      "Test accuracy: 90.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [data_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (data_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 139.641937\n",
      "Minibatch accuracy: 21.1%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 1500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 2000: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 2500: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 3000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 3500: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 4000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 4500: nan\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 5000: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 5500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 6000: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 6500: nan\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 7000: nan\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 7500: nan\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 8000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 8500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 9000: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 9500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 10000: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 10500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 11000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 11500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 12000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 12500: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 13000: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 13500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 14000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 14500: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 15000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 15500: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 16000: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 16500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 17000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 17500: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 18000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Test accuracy: 8.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, data_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [data_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (data_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 657.478577\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 1000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 1500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 2000: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 2500: nan\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 3000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 3500: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 4000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 4500: nan\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 5000: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 5500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 6000: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 6500: nan\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 7000: nan\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 7500: nan\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 8000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 8500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 9000: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 9500: nan\n",
      "Minibatch accuracy: 4.7%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 10000: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 10500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 11000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 11500: nan\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 12000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 12500: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 13000: nan\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 13500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 14000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 14500: nan\n",
      "Minibatch accuracy: 5.5%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 15000: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 15500: nan\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 16000: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 16500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 17000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 17500: nan\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 18000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 18500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 19000: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 19500: nan\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 7.7%\n",
      "Minibatch loss at step 20000: nan\n",
      "Minibatch accuracy: 3.9%\n",
      "Validation accuracy: 7.7%\n",
      "Test accuracy: 8.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, data_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [data_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, data_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [data_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    pool1 = tf.nn.max_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    conv2 = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    bias2 = tf.nn.relu(conv2 + layer2_biases)\n",
    "    pool2 = tf.nn.max_pool(bias2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN below is loosely inspired by the LeNet5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, data_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  size3 = ((data_size - patch_size + 1) // 2 - patch_size + 1) // 2\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [size3 * size3 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    # C1 input 28 x 28\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # S2 input 24 x 24\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # C3 input 12 x 12\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    # S4 input 8 x 8\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # F6 input 4 x 4\n",
    "    shape = pool4.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is good, but not as good as the 3-layer network from the previous assignment.\n",
    "\n",
    "The next version of the net uses dropout and learning rate decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "beta_regul = 1e-3\n",
    "drop_out = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  size3 = ((image_size - patch_size + 1) // 2 - patch_size + 1) // 2\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [size3 * size3 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob):\n",
    "    # C1 input 28 x 28\n",
    "    conv1 = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias1 = tf.nn.relu(conv1 + layer1_biases)\n",
    "    # S2 input 24 x 24\n",
    "    pool2 = tf.nn.avg_pool(bias1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # C3 input 12 x 12\n",
    "    conv3 = tf.nn.conv2d(pool2, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "    bias3 = tf.nn.relu(conv3 + layer2_biases)\n",
    "    # S4 input 8 x 8\n",
    "    pool4 = tf.nn.avg_pool(bias3, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "    # F5 input 4 x 4\n",
    "    shape = pool4.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden5 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    # F6\n",
    "    drop5 = tf.nn.dropout(hidden5, keep_prob)\n",
    "    hidden6 = tf.nn.relu(tf.matmul(hidden5, layer4_weights) + layer4_biases)\n",
    "    drop6 = tf.nn.dropout(hidden6, keep_prob)\n",
    "    return tf.matmul(drop6, layer5_weights) + layer5_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, drop_out)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.85, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset, 1.0))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 5001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)\n",
    "    \n",
    "for i in range(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the accuracy is worst. This net has many meta parameters and I don't feel comfortable in tuning them randomly. I should probably change the depth and make it different between the layers, since it looks like the increasing number of feature maps is a key design item.\n",
    "\n",
    "I will do so in a next version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
