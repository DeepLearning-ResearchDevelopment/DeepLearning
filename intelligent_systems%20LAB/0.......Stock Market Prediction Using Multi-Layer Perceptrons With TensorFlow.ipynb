{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Prediction Using Multi-Layer Perceptrons With TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Setup\n",
    "\n",
    "The data used in this post was collected from finance.yahoo.com. The data consists of historical stock data from Yahoo Inc. over the period of the 12th of April 1996 to the 19th of April 2016. The data can be downloaded as a CSV file from the provided link. To pre-process the data for the neural network, first transform the dates into integer values using LibreOfficeâ€™s DATEVALUE function. A screen-shot of the transformed data can be seen as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filePath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d9851b4cd22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilePath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'yahoostock.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filePath' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as mpl\n",
    "from sklearn.preprocessing import scale\n",
    " \n",
    "pth = filePath + 'yahoostock.csv'\n",
    "A = np.loadtxt(pth, delimiter=\",\", skiprows=1, usecols=(1, 4))\n",
    "A = scale(A)\n",
    "#y is the dependent variable\n",
    "y = A[:, 1].reshape(-1, 1)\n",
    "#A contains the independent variable\n",
    "A = A[:, 0].reshape(-1, 1)\n",
    "#Plot the high value of the stock price\n",
    "mpl.plot(A[:, 0], y[:, 0])\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP class that will be used follows a simple interface similar to that of the python scikit-learn library. The source code is available here. The interface is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the MLP to the data\n",
    "#param A: numpy matrix where each row is a sample\n",
    "#param y: numpy matrix of target values\n",
    "def fit(self, A, y):\n",
    " \n",
    "#Predict the output given the input (only run after calling fit)\n",
    "#param A: The input values for which to predict outputs\n",
    "#return: The predicted output values (one row per input sample)\n",
    "def predict(self, A):\n",
    " \n",
    "#Predicts the ouputs for input A and then computes the RMSE between\n",
    "#The predicted values and the actualy values\n",
    "#param A: The input values for which to predict outputs\n",
    "#param y: The actual target values\n",
    "#return: The RMSE\n",
    "def score(self, A, y):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create an MLPR object. This can be done as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of neurons in the input layer\n",
    "i = 1\n",
    "#Number of neurons in the output layer\n",
    "o = 1\n",
    "#Number of neurons in the hidden layers\n",
    "h = 32\n",
    "#The list of layer sizes\n",
    "layers = [i, h, h, h, h, h, h, h, h, h, o]\n",
    "mlpr = MLPR(layers, maxItr = 1000, tol = 0.40, reg = 0.001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, an MLPR object will be initialized with the given layer sizes, a training iteration limit of 1000, an error tolerance of 0.40 (for the RMSE), regularization weight of 0.001, and verbose output enabled. The source code for the MLPR class shows how this is accomplished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the MLP variables for TF graph\n",
    "#_X: The input matrix\n",
    "#_W: The weight matrices\n",
    "#_B: The bias vectors\n",
    "#_AF: The activation function\n",
    "def _CreateMLP(_X, _W, _B, _AF):\n",
    "    n = len(_W)\n",
    "    for i in range(n - 1):\n",
    "        _X = _AF(tf.matmul(_X, _W[i]) + _B[i])\n",
    "    return tf.matmul(_X, _W[n - 1]) + _B[n - 1]\n",
    " \n",
    "#Add L2 regularizers for the weight and bias matrices\n",
    "#_W: The weight matrices\n",
    "#_B: The bias matrices\n",
    "#return: tensorflow variable representing l2 regularization cost\n",
    "def _CreateL2Reg(_W, _B):\n",
    "    n = len(_W)\n",
    "    regularizers = tf.nn.l2_loss(_W[0]) + tf.nn.l2_loss(_B[0])\n",
    "    for i in range(1, n):\n",
    "        regularizers += tf.nn.l2_loss(_W[i]) + tf.nn.l2_loss(_B[i])\n",
    "    return regularizers\n",
    " \n",
    "#Create weight and bias vectors for an MLP\n",
    "#layers: The number of neurons in each layer (including input and output)\n",
    "#return: A tuple of lists of the weight and bias matrices respectively\n",
    "def _CreateVars(layers):\n",
    "    weight = []\n",
    "    bias = []\n",
    "    n = len(layers)\n",
    "    for i in range(n - 1):\n",
    "        #Fan-in for layer; used as standard dev\n",
    "        lyrstd = np.sqrt(1.0 / layers[i])\n",
    "        curW = tf.Variable(tf.random_normal([layers[i], layers[i + 1]], stddev = lyrstd))\n",
    "        weight.append(curW)\n",
    "        curB = tf.Variable(tf.random_normal([layers[i + 1]], stddev = lyrstd))\n",
    "        bias.append(curB)\n",
    "    return (weight, bias)\n",
    " \n",
    "...\n",
    " \n",
    "#The constructor\n",
    "#param layers: A list of layer sizes\n",
    "#param actvFn: The activation function to use: 'tanh', 'sig', or 'relu'\n",
    "#param learnRate: The learning rate parameter\n",
    "#param decay: The decay parameter\n",
    "#param maxItr: Maximum number of training iterations\n",
    "#param tol: Maximum error tolerated\n",
    "#param batchSize: Size of training batches to use (use all if None)\n",
    "#param verbose: Print training information\n",
    "#param reg: Regularization weight\n",
    "def __init__(self, layers, actvFn = 'tanh', learnRate = 0.001, decay = 0.9, maxItr = 2000,\n",
    "             tol = 1e-2, batchSize = None, verbose = False, reg = 0.001):\n",
    "    #Parameters\n",
    "    self.tol = tol\n",
    "    self.mItr = maxItr\n",
    "    self.vrbse = verbose\n",
    "    self.batSz = batchSize\n",
    "    #Input size\n",
    "    self.x = tf.placeholder(\"float\", [None, layers[0]])\n",
    "    #Output size\n",
    "    self.y = tf.placeholder(\"float\", [None, layers[-1]])\n",
    "    #Setup the weight and bias variables\n",
    "    weight, bias = _CreateVars(layers)\n",
    "    #Create the tensorflow MLP model\n",
    "    self.pred = _CreateMLP(self.x, weight, bias, _GetActvFn(actvFn))\n",
    "    #Use L2 as the cost function\n",
    "    self.loss = tf.reduce_sum(tf.nn.l2_loss(self.pred - self.y))\n",
    "    #Use regularization to prevent over-fitting\n",
    "    if(reg is not None):\n",
    "        self.loss += _CreateL2Reg(weight, bias) * reg\n",
    "    #Use ADAM method to minimize the loss function\n",
    "    self.optmzr = tf.train.AdamOptimizer(learning_rate=learnRate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the MLP is trained with the Yahoo stock data. A hold-out period is used to assess how well the MLP is performing. This can be accomplished as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Length of the hold-out period\n",
    "nDays = 5\n",
    "n = len(A)\n",
    "#Learn the data\n",
    "mlpr.fit(A[0:(n-nDays)], y[0:(n-nDays)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the fit function is called, the actual training process begins. First, a tensorflow session must be created and all variables defined in the constructor must be initialized. Then, training iterations are performed up to the iteration limit provided, the weights are updated, and the error is recorded. The feed_dict parameter specifies the values of our inputs (x) and outputs (y). If the error falls below the tolerance level, training is completed, otherwise the maximum number of iterations is exhausted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the MLP to the data\n",
    "#param A: numpy matrix where each row is a sample\n",
    "#param y: numpy matrix of target values\n",
    "def fit(self, A, y):\n",
    "    m = len(A)\n",
    "    #Start the tensorflow session and initializer\n",
    "    #all variables\n",
    "    self.sess = tf.Session()\n",
    "    init = tf.initialize_all_variables()\n",
    "    self.sess.run(init)\n",
    "    #Begin training\n",
    "    for i in range(self.mItr):\n",
    "        #Batch mode or all at once\n",
    "        if(self.batSz is None):\n",
    "            self.sess.run(self.optmzr, feed_dict={self.x:A, self.y:y})\n",
    "        else:\n",
    "            for j in range(0, m, self.batSz):\n",
    "                batA, batY = _NextBatch(A, y, j, self.batSz)\n",
    "                self.sess.run(self.optmzr, feed_dict={self.x:batA, self.y:batY})\n",
    "        err = np.sqrt(self.sess.run(self.loss, feed_dict={self.x:A, self.y:y}) * 2.0 / m)\n",
    "        if(self.vrbse):\n",
    "            print(\"Iter \" + str(i + 1) + \": \" + str(err))\n",
    "        if(err < self.tol):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the MLP network trained, prediction can be performed and the results plotted using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Begin prediction\n",
    "yHat = mlpr.predict(A)\n",
    "#Plot the results\n",
    "mpl.plot(A, y, c='#b0403f')\n",
    "mpl.plot(A, yHat, c='#5aa9ab')\n",
    "mpl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
