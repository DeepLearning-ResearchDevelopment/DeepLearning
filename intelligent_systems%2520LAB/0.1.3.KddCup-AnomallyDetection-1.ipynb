{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KddCup-AnomallyDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_uniq.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_as_list = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Toplam satir sayisi : ', 1048575)\n"
     ]
    }
   ],
   "source": [
    "def file_len(adress):\n",
    "  f = open(adress)\n",
    "  nr_of_lines = sum(1 for line in f)\n",
    "  f.close()\n",
    "  return nr_of_lines\n",
    "\n",
    "number_of_lines = file_len('data_uniq.csv')\n",
    "print('Toplam satir sayisi : ', number_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fullData = np.empty(shape=[number_of_lines, 42])\n",
    "\n",
    "for i in range(number_of_lines):\n",
    "    for j in range(42):\n",
    "        fullData[i][j] = data_as_list[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set for class 0: 247256\n",
      "Size of data set for class 1: 35\n",
      "Size of data set for class 2: 36\n",
      "Size of data set for class 3: 13295\n",
      "Size of data set for class 4: 787953\n"
     ]
    }
   ],
   "source": [
    "sayac = np.zeros(5, dtype=int)\n",
    "for i in range(number_of_lines):\n",
    "    sayac[fullData[i][41]] +=  1\n",
    "    \n",
    "for i in range(5):\n",
    "    print('Size of data set for class ' + str(i) + ': ' + str(sayac[i]))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset normalized (x-mean)/std \n",
    "But the column shows class number is not normalizated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isl-eyup/.local/lib/python2.7/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "meanArray = np.mean(fullData, axis=0)\n",
    "stdArray = np.std(fullData, axis=0)\n",
    "fullDataNormalized = np.empty(shape=[fullData.shape[0], fullData.shape[1]])\n",
    "print(fullDataNormalized.shape)\n",
    "\n",
    "\n",
    "for i in range(fullData.shape[0]):\n",
    "    for j in range(fullData.shape[1]-1):\n",
    "        fullDataNormalized[i][j] = (fullData[i][j] - meanArray[j]) / stdArray[j]\n",
    "\n",
    "for i in range(fullData.shape[0]):\n",
    "    fullDataNormalized[i][41] = fullData[i][41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fullDataNormalized csv file  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"Array save to csv file \"\"\"\n",
    "np.savetxt(\n",
    "    'fullDataNormalized.csv', # file name\n",
    "    fullDataNormalized,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "\n",
    "\n",
    "print(\"Saved fullDataNormalized csv file  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dosSet = np.empty(shape=[sayac[0], fullDataNormalized.shape[1]])\n",
    "u2rSet = np.empty(shape=[sayac[1], fullDataNormalized.shape[1]])\n",
    "r2lSet = np.empty(shape=[sayac[2], fullDataNormalized.shape[1]])\n",
    "probeSet = np.empty(shape=[sayac[3], fullDataNormalized.shape[1]])\n",
    "trainingSet = np.empty(shape=[sayac[4], fullDataNormalized.shape[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set created with normal class data\n",
      "(787953, 42)\n",
      "dos attack set created \n",
      "(247256, 42)\n",
      "u2r attack set created \n",
      "(35, 42)\n",
      "r2l attack set created \n",
      "(36, 42)\n",
      "probe attack set created \n",
      "(13295, 42)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "tcount = 0 \n",
    "tcount2 = 0 \n",
    "tcount3 = 0 \n",
    "tcount4 = 0 \n",
    "\n",
    "for i in range(fullDataNormalized.shape[0]):\n",
    "    if fullDataNormalized[i][41] == 0: # dos attacks\n",
    "        for k in range(fullDataNormalized.shape[1]):\n",
    "            dosSet[tcount][k] = fullDataNormalized[i][k]\n",
    "        tcount += 1\n",
    "    if fullDataNormalized[i][41] == 1: # u2l attacks\n",
    "        for k in range(fullDataNormalized.shape[1]):\n",
    "            u2rSet[tcount2][k] = fullDataNormalized[i][k]\n",
    "        tcount2 += 1\n",
    "    if fullDataNormalized[i][41] == 2: # r2l attacks\n",
    "        for k in range(fullDataNormalized.shape[1]):\n",
    "            r2lSet[tcount3][k] = fullDataNormalized[i][k]\n",
    "        tcount3 += 1\n",
    "    if fullDataNormalized[i][41] == 3: # probe attacks\n",
    "        for k in range(fullDataNormalized.shape[1]):\n",
    "            probeSet[tcount4][k] = fullDataNormalized[i][k]\n",
    "        tcount4 += 1                \n",
    "    if fullDataNormalized[i][41] == 4 : # normal \n",
    "        for k in range(fullDataNormalized.shape[1]):\n",
    "            trainingSet[count][k] = fullDataNormalized[i][k]\n",
    "        count += 1\n",
    "        \n",
    "print(\"Training set created with normal class data\")    \n",
    "print(trainingSet.shape) \n",
    "\n",
    "\n",
    "print(\"dos attack set created \")    \n",
    "print(dosSet.shape)      \n",
    "\n",
    "print(\"u2r attack set created \")    \n",
    "print(u2rSet.shape)\n",
    "\n",
    "print(\"r2l attack set created \")    \n",
    "print(r2lSet.shape)\n",
    "\n",
    "print(\"probe attack set created \")    \n",
    "print(probeSet.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dosSet csv file  \n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\n",
    "    'dosSet.csv', # file name\n",
    "    dosSet,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "print(\"Saved dosSet csv file  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved u2rSet csv file  \n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\n",
    "    'u2rSet.csv', # file name\n",
    "    u2rSet,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "print(\"Saved u2rSet csv file  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved r2lSet csv file  \n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\n",
    "    'r2lSet.csv', # file name\n",
    "    r2lSet,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "print(\"Saved r2lSet csv file  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved probeSet csv file  \n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\n",
    "    'probeSet.csv', # file name\n",
    "    probeSet,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "print(\"Saved probeSet csv file  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trainingSetNormalized csv file  \n",
      "Saved testSetNormalized csv file  \n"
     ]
    }
   ],
   "source": [
    "np.savetxt(\n",
    "    'trainingSetNormalized.csv', # file name\n",
    "    trainingSet,    # array to save\n",
    "    fmt='%.5f',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n',           # new line character\n",
    "    comments='# ') \n",
    "print(\"Saved trainingSetNormalized csv file  \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 42\n",
    "n_hidden_1 = 36 # 1st layer num features\n",
    "n_hidden_2 = 32 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3939)\n",
      "('Epoch:', '0001', 'cost=', 'nan')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0301', 'cost=', 'nan')\n",
      "('Step Count :', 300)\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 300\n",
    "num_steps=3001\n",
    "total_batch = int(trainingSet.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainingSet.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainingSet[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
