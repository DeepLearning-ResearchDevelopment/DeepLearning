{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KddCup-AnomallyDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized Dataset saved as fullDataNormalized.csv file\n",
    "\n",
    "Normalized Trainingset saved as trainingSetNormalized.csv file (Normal data)\n",
    "\n",
    "Dos:dosSet.csv \n",
    "u2r:u2r.csv\n",
    "r2l:r2l.csv\n",
    "probe:probe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingSetNormalized.csv file opened\n",
      "dosSet.csv file opened\n",
      "u2rSet.csv file opened\n",
      "r2lSet.csv file opened\n",
      "probeSet.csv file opened\n",
      "OK!\n"
     ]
    }
   ],
   "source": [
    "with open('trainingSetNormalized.csv', 'rb') as f1:\n",
    "    reader = csv.reader(f1)\n",
    "    trainList = list(reader)\n",
    "f1.close()    \n",
    "print(\"trainingSetNormalized.csv file opened\")\n",
    "with open('dosSet.csv', 'rb') as f2:\n",
    "    reader = csv.reader(f2)\n",
    "    dosList = list(reader) \n",
    "f2.close()    \n",
    "print(\"dosSet.csv file opened\")\n",
    "with open('u2rSet.csv', 'rb') as f3:\n",
    "    reader = csv.reader(f3)\n",
    "    u2rList = list(reader) \n",
    "f3.close()        \n",
    "print(\"u2rSet.csv file opened\")\n",
    "with open('r2lSet.csv', 'rb') as f4:\n",
    "    reader = csv.reader(f4)\n",
    "    r2lList = list(reader)\n",
    "f4.close()    \n",
    "print(\"r2lSet.csv file opened\")\n",
    "with open('probeSet.csv', 'rb') as f5:\n",
    "    reader = csv.reader(f5)\n",
    "    probeList = list(reader) \n",
    "f5.close()    \n",
    "print(\"probeSet.csv file opened\")\n",
    "\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def file_len(adress):\n",
    "    f = open(adress)\n",
    "    nr_of_lines = sum(1 for line in f)\n",
    "    f.close()\n",
    "    return nr_of_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total length of train data : ', 787953)\n",
      "('Total length of dos data : ', 247256)\n",
      "('Total length of u2r data : ', 35)\n",
      "('Total length of r2l data : ', 36)\n",
      "('Total length of probe data : ', 13295)\n"
     ]
    }
   ],
   "source": [
    "trainLen = file_len('trainingSetNormalized.csv')\n",
    "print(\"Total length of train data : \", trainLen)\n",
    "\n",
    "dosLen = file_len('dosSet.csv')\n",
    "print(\"Total length of dos data : \", dosLen)\n",
    "\n",
    "u2rLen = file_len('u2rSet.csv')\n",
    "print(\"Total length of u2r data : \", u2rLen)\n",
    "\n",
    "r2lLen = file_len('r2lSet.csv')\n",
    "print(\"Total length of r2l data : \", r2lLen)\n",
    "\n",
    "probeLen = file_len('probeSet.csv')\n",
    "print(\"Total length of probe data : \", probeLen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TrainingSet : ', (787953, 41))\n",
      "('TestSet : ', (13295, 41))\n"
     ]
    }
   ],
   "source": [
    "# test with probe\n",
    "trainingSet = np.empty(shape=[trainLen, 41])\n",
    "testSet = np.empty(shape=[probeLen, 41]) \n",
    "\n",
    "for i in range(trainLen):\n",
    "    for j in range(41):\n",
    "        trainingSet[i][j] = trainList[i][j]\n",
    "print(\"TrainingSet : \", trainingSet.shape)     \n",
    "for i in range(trainLen):\n",
    "    trainingSet[i][19] = 0\n",
    "\n",
    "for i in range(probeLen):\n",
    "    for j in range(41):\n",
    "        testSet[i][j] = probeList[i][j] \n",
    "for i in range(probeLen):\n",
    "    testSet[i][19] = 0\n",
    "print(\"TestSet : \", testSet.shape)        \n",
    "#index 19 is nan because std is 0,,, so in matrix' 19th index element = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# test with dos\n",
    "trainingSet = np.empty(shape=[trainLen, 41])\n",
    "testSet = np.empty(shape=[dosLen, 41]) \n",
    "\n",
    "for i in range(trainLen):\n",
    "    for j in range(41):\n",
    "        trainingSet[i][j] = trainList[i][j]\n",
    "print(\"TrainingSet : \", trainingSet.shape)     \n",
    "for i in range(trainLen):\n",
    "    trainingSet[i][19] = 0\n",
    "\n",
    "for i in range(dosLen):\n",
    "    for j in range(41):\n",
    "        testSet[i][j] = testList[i][j] \n",
    "for i in range(dosLen):\n",
    "    testSet[i][19] = 0\n",
    "print(\"TestSet : \", testSet.shape)        \n",
    "#index 19 is nan because std is 0,,, so in matrix' 19th index element = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 500\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 39 # 1st layer num features\n",
    "n_hidden_2 = 37 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 1575)\n",
      "('Step Count :', 0, 'Epoch:', '0001', 'cost=', '1.914053321')\n",
      "('Step Count :', 400, 'Epoch:', '0401', 'cost=', '0.106771439')\n",
      "('Step Count :', 800, 'Epoch:', '0801', 'cost=', '0.109914325')\n",
      "('Step Count :', 1200, 'Epoch:', '1201', 'cost=', '0.273107499')\n",
      "('Step Count :', 1600, 'Epoch:', '1601', 'cost=', '0.268151432')\n",
      "('Step Count :', 2000, 'Epoch:', '2001', 'cost=', '0.102226719')\n",
      "('Step Count :', 2400, 'Epoch:', '2401', 'cost=', '0.124305092')\n",
      "('Step Count :', 2800, 'Epoch:', '2801', 'cost=', '0.610939026')\n",
      "('Step Count :', 3200, 'Epoch:', '3201', 'cost=', '0.208154112')\n",
      "('Step Count :', 3600, 'Epoch:', '3601', 'cost=', '0.105424687')\n",
      "('Step Count :', 4000, 'Epoch:', '4001', 'cost=', '0.108449787')\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainingSet.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainingSet.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainingSet[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Step Count :\" ,step, \"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        \n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: testSet[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 41)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_decode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.67600000e-02,  -3.77861000e+00,  -2.48153000e+00,\n",
       "        -5.50310000e-01,  -2.18000000e-03,  -3.33000000e-03,\n",
       "        -4.98000000e-03,  -3.13500000e-02,  -1.71000000e-03,\n",
       "        -5.40100000e-02,  -8.38000000e-03,  -1.30924000e+00,\n",
       "        -6.75000000e-03,  -1.66600000e-02,  -9.16000000e-03,\n",
       "        -1.19300000e-02,  -1.95300000e-02,  -1.82700000e-02,\n",
       "        -5.83700000e-02,   0.00000000e+00,  -9.80000000e-04,\n",
       "        -6.16400000e-02,   6.33930000e-01,   3.08658000e+00,\n",
       "        -4.93780000e-01,  -4.94700000e-01,  -2.95180000e-01,\n",
       "        -2.95580000e-01,   5.71720000e-01,  -2.70410000e-01,\n",
       "        -4.42400000e-01,   9.08900000e-01,   8.56020000e-01,\n",
       "         7.61810000e-01,  -3.71170000e-01,   4.18069000e+00,\n",
       "        -3.91840000e-01,  -4.95270000e-01,  -4.93190000e-01,\n",
       "        -3.06120000e-01,  -3.02600000e-01])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.00228180e-06,   3.35547193e-05,   1.59903686e-06,\n",
       "         5.88455350e-06,   4.86902463e-05,   2.98202591e-04,\n",
       "         7.99740592e-05,   8.51102232e-06,   6.04503963e-04,\n",
       "         1.03884468e-05,   3.89677698e-05,   7.87762880e-01,\n",
       "         4.59654511e-05,   5.14870189e-05,   4.54760244e-04,\n",
       "         4.27797379e-04,   2.76705487e-05,   1.80990290e-04,\n",
       "         2.28837901e-03,   4.85179153e-05,   7.37562414e-06,\n",
       "         2.05380256e-05,   8.88946772e-07,   2.17084438e-04,\n",
       "         5.31000410e-07,   7.56725422e-06,   2.21778741e-04,\n",
       "         2.21505252e-06,   6.37791634e-01,   2.51313686e-05,\n",
       "         4.01417338e-07,   5.31951741e-07,   8.93490911e-01,\n",
       "         7.71922886e-01,   1.66875748e-06,   9.99683619e-01,\n",
       "         1.10073231e-01,   1.96198494e-06,   6.72947863e-06,\n",
       "         8.62122700e-03,   4.34382207e-04], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_decode[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('STEP :', '0001', 'cost = ', '7.800394351')\n",
      "('STEP :', '0051', 'cost = ', '7.599238015')\n",
      "('STEP :', '0101', 'cost = ', '7.593955417')\n",
      "('STEP :', '0151', 'cost = ', '7.593150545')\n",
      "('STEP :', '0201', 'cost = ', '1.225110118')\n",
      "('STEP :', '0251', 'cost = ', '7.631004735')\n",
      "('STEP :', '0301', 'cost = ', '7.614301281')\n",
      "('STEP :', '0351', 'cost = ', '1.234198174')\n",
      "('STEP :', '0401', 'cost = ', '1.252255941')\n",
      "('STEP :', '0451', 'cost = ', '1.254657137')\n",
      "('STEP :', '0501', 'cost = ', '1.262540130')\n",
      "('STEP :', '0551', 'cost = ', '3.109626238')\n",
      "('STEP :', '0601', 'cost = ', '1.441065535')\n",
      "('STEP :', '0651', 'cost = ', '2.440202469')\n",
      "('STEP :', '0701', 'cost = ', '2.526070464')\n",
      "('STEP :', '0751', 'cost = ', '1.701296291')\n",
      "('STEP :', '0801', 'cost = ', '2.503368418')\n",
      "('STEP :', '0851', 'cost = ', '1.385560175')\n",
      "('STEP :', '0901', 'cost = ', '1.385867292')\n",
      "('STEP :', '0951', 'cost = ', '1.441275451')\n",
      "('STEP :', '1001', 'cost = ', '2.585022521')\n",
      "('STEP :', '1051', 'cost = ', '1.444903710')\n",
      "('STEP :', '1101', 'cost = ', '2.523780750')\n",
      "('STEP :', '1151', 'cost = ', '2.910556951')\n",
      "('STEP :', '1201', 'cost = ', '1.375527497')\n",
      "('STEP :', '1251', 'cost = ', '2.573313093')\n",
      "('STEP :', '1301', 'cost = ', '1.442739859')\n",
      "('STEP :', '1351', 'cost = ', '2.528839040')\n",
      "('STEP :', '1401', 'cost = ', '2.551963227')\n",
      "('STEP :', '1451', 'cost = ', '2.522452224')\n",
      "('STEP :', '1501', 'cost = ', '2.504089415')\n",
      "('STEP :', '1551', 'cost = ', '1.415300308')\n",
      "('STEP :', '1601', 'cost = ', '2.569446313')\n",
      "('STEP :', '1651', 'cost = ', '2.614182285')\n",
      "('STEP :', '1701', 'cost = ', '1.433083072')\n",
      "('STEP :', '1751', 'cost = ', '2.590398418')\n",
      "('STEP :', '1801', 'cost = ', '1.403690314')\n",
      "('STEP :', '1851', 'cost = ', '2.634069096')\n",
      "('STEP :', '1901', 'cost = ', '2.537210008')\n",
      "('STEP :', '1951', 'cost = ', '2.552717645')\n",
      "('STEP :', '2001', 'cost = ', '2.534016758')\n",
      "('STEP :', '2051', 'cost = ', '2.642427477')\n",
      "('STEP :', '2101', 'cost = ', '1.405036605')\n",
      "('STEP :', '2151', 'cost = ', '2.574061889')\n",
      "('STEP :', '2201', 'cost = ', '2.604171028')\n",
      "('STEP :', '2251', 'cost = ', '2.541818599')\n",
      "('STEP :', '2301', 'cost = ', '2.555814026')\n",
      "('STEP :', '2351', 'cost = ', '2.538103583')\n",
      "('STEP :', '2401', 'cost = ', '2.640987981')\n",
      "('STEP :', '2451', 'cost = ', '1.404405050')\n",
      "('STEP :', '2501', 'cost = ', '2.574857529')\n",
      "('STEP :', '2551', 'cost = ', '2.623560781')\n",
      "('STEP :', '2601', 'cost = ', '2.535238998')\n",
      "('STEP :', '2651', 'cost = ', '2.564971664')\n",
      "('STEP :', '2701', 'cost = ', '2.547997042')\n",
      "('STEP :', '2751', 'cost = ', '1.494783992')\n",
      "('STEP :', '2801', 'cost = ', '1.429702249')\n",
      "('STEP :', '2851', 'cost = ', '1.495545588')\n",
      "('STEP :', '2901', 'cost = ', '2.637997511')\n",
      "('STEP :', '2951', 'cost = ', '1.429016700')\n",
      "('Average cost : ', 2.6486874081387093)\n"
     ]
    }
   ],
   "source": [
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(testSet[i] - encode_decode[i], 2))\n",
    "    if step % 50 == 0 :\n",
    "        print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training normal data  80% and test other normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TrainingSet : ', (640000, 41))\n",
      "('TestSet : ', (140000, 41))\n"
     ]
    }
   ],
   "source": [
    "trainingSet2 = np.empty(shape=[640000, 41])\n",
    "testSet2 = np.empty(shape=[140000,41])\n",
    "for i in range(trainingSet2.shape[0]):\n",
    "    for j in range(41):\n",
    "        trainingSet2[i][j] = trainingSet[i][j]\n",
    "print(\"TrainingSet : \", trainingSet2.shape)     \n",
    "for i in range(trainingSet2.shape[0]):\n",
    "    trainingSet[i][19] = 0\n",
    "\n",
    "for i in range(testSet2.shape[0]):\n",
    "    for j in range(41):\n",
    "        testSet2[i][j] = trainingSet[i+640000][j] \n",
    "for i in range(testSet2.shape[0]):\n",
    "    testSet[i][19] = 0\n",
    "print(\"TestSet : \", testSet2.shape)        \n",
    "#index 19 is nan because std is 0,,, so in matrix' 19th index element = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 40 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3200)\n",
      "('Epoch:', '0001', 'cost=', '2.091668367')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.932155490')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.104474127')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '0.089258589')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.094946451')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '0.106285311')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '0.812773526')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.095268220')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '1.068478584')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.926778436')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.103599392')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainingSet2.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainingSet2.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainingSet[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: testSet2[:100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('STEP :', '0001', 'cost = ', '0.278274276')\n",
      "('STEP :', '0002', 'cost = ', '1.893928860')\n",
      "('STEP :', '0003', 'cost = ', '0.494011689')\n",
      "('STEP :', '0004', 'cost = ', '0.148261872')\n",
      "('STEP :', '0005', 'cost = ', '0.148752706')\n",
      "('STEP :', '0006', 'cost = ', '0.137089552')\n",
      "('STEP :', '0007', 'cost = ', '0.114114417')\n",
      "('STEP :', '0008', 'cost = ', '0.143503141')\n",
      "('STEP :', '0009', 'cost = ', '0.118217732')\n",
      "('STEP :', '0010', 'cost = ', '0.291572173')\n",
      "('STEP :', '0011', 'cost = ', '0.146638538')\n",
      "('STEP :', '0012', 'cost = ', '0.264558499')\n",
      "('STEP :', '0013', 'cost = ', '0.221340076')\n",
      "('STEP :', '0014', 'cost = ', '0.262439112')\n",
      "('STEP :', '0015', 'cost = ', '0.418259419')\n",
      "('STEP :', '0016', 'cost = ', '0.323063670')\n",
      "('STEP :', '0017', 'cost = ', '0.178475629')\n",
      "('STEP :', '0018', 'cost = ', '0.128138104')\n",
      "('STEP :', '0019', 'cost = ', '0.134167231')\n",
      "('STEP :', '0020', 'cost = ', '1.993381003')\n",
      "('STEP :', '0021', 'cost = ', '0.428362813')\n",
      "('STEP :', '0022', 'cost = ', '0.111898813')\n",
      "('STEP :', '0023', 'cost = ', '0.123924251')\n",
      "('STEP :', '0024', 'cost = ', '0.184809761')\n",
      "('STEP :', '0025', 'cost = ', '0.117206757')\n",
      "('STEP :', '0026', 'cost = ', '0.146634801')\n",
      "('STEP :', '0027', 'cost = ', '0.156317794')\n",
      "('STEP :', '0028', 'cost = ', '0.167476330')\n",
      "('STEP :', '0029', 'cost = ', '0.139700881')\n",
      "('STEP :', '0030', 'cost = ', '0.129345384')\n",
      "('STEP :', '0031', 'cost = ', '0.112090600')\n",
      "('STEP :', '0032', 'cost = ', '0.117768482')\n",
      "('STEP :', '0033', 'cost = ', '0.275221690')\n",
      "('STEP :', '0034', 'cost = ', '0.135487184')\n",
      "('STEP :', '0035', 'cost = ', '0.126292636')\n",
      "('STEP :', '0036', 'cost = ', '0.294188328')\n",
      "('STEP :', '0037', 'cost = ', '0.119737507')\n",
      "('STEP :', '0038', 'cost = ', '0.116055598')\n",
      "('STEP :', '0039', 'cost = ', '0.266233425')\n",
      "('STEP :', '0040', 'cost = ', '0.278671632')\n",
      "('STEP :', '0041', 'cost = ', '0.334850556')\n",
      "('STEP :', '0042', 'cost = ', '0.114601311')\n",
      "('STEP :', '0043', 'cost = ', '0.118247517')\n",
      "('STEP :', '0044', 'cost = ', '0.147767134')\n",
      "('STEP :', '0045', 'cost = ', '0.280872078')\n",
      "('STEP :', '0046', 'cost = ', '0.273207834')\n",
      "('STEP :', '0047', 'cost = ', '0.129904741')\n",
      "('STEP :', '0048', 'cost = ', '0.287561153')\n",
      "('STEP :', '0049', 'cost = ', '0.119026437')\n",
      "('STEP :', '0050', 'cost = ', '0.135181833')\n",
      "('STEP :', '0051', 'cost = ', '0.246791903')\n",
      "('STEP :', '0052', 'cost = ', '0.265611712')\n",
      "('STEP :', '0053', 'cost = ', '0.112858050')\n",
      "('STEP :', '0054', 'cost = ', '0.152389838')\n",
      "('STEP :', '0055', 'cost = ', '0.288182345')\n",
      "('STEP :', '0056', 'cost = ', '0.118672618')\n",
      "('STEP :', '0057', 'cost = ', '0.273115915')\n",
      "('STEP :', '0058', 'cost = ', '0.112975997')\n",
      "('STEP :', '0059', 'cost = ', '0.112997876')\n",
      "('STEP :', '0060', 'cost = ', '0.262920864')\n",
      "('STEP :', '0061', 'cost = ', '0.134168408')\n",
      "('STEP :', '0062', 'cost = ', '0.266912683')\n",
      "('STEP :', '0063', 'cost = ', '0.331439224')\n",
      "('STEP :', '0064', 'cost = ', '0.123857982')\n",
      "('STEP :', '0065', 'cost = ', '0.140286400')\n",
      "('STEP :', '0066', 'cost = ', '0.193934976')\n",
      "('STEP :', '0067', 'cost = ', '0.126782314')\n",
      "('STEP :', '0068', 'cost = ', '0.113202837')\n",
      "('STEP :', '0069', 'cost = ', '0.157070470')\n",
      "('STEP :', '0070', 'cost = ', '0.144803991')\n",
      "('STEP :', '0071', 'cost = ', '2.010843275')\n",
      "('STEP :', '0072', 'cost = ', '0.302080587')\n",
      "('STEP :', '0073', 'cost = ', '0.197211363')\n",
      "('STEP :', '0074', 'cost = ', '0.127848584')\n",
      "('STEP :', '0075', 'cost = ', '0.119549523')\n",
      "('STEP :', '0076', 'cost = ', '0.114047729')\n",
      "('STEP :', '0077', 'cost = ', '0.114103046')\n",
      "('STEP :', '0078', 'cost = ', '0.165154391')\n",
      "('STEP :', '0079', 'cost = ', '0.261995100')\n",
      "('STEP :', '0080', 'cost = ', '0.277529573')\n",
      "('STEP :', '0081', 'cost = ', '0.124750659')\n",
      "('STEP :', '0082', 'cost = ', '0.118680838')\n",
      "('STEP :', '0083', 'cost = ', '0.262382818')\n",
      "('STEP :', '0084', 'cost = ', '0.271061077')\n",
      "('STEP :', '0085', 'cost = ', '0.272937800')\n",
      "('STEP :', '0086', 'cost = ', '0.141969450')\n",
      "('STEP :', '0087', 'cost = ', '0.371133304')\n",
      "('STEP :', '0088', 'cost = ', '0.299692916')\n",
      "('STEP :', '0089', 'cost = ', '0.272831372')\n",
      "('STEP :', '0090', 'cost = ', '0.361072962')\n",
      "('STEP :', '0091', 'cost = ', '0.311041938')\n",
      "('STEP :', '0092', 'cost = ', '0.111412077')\n",
      "('STEP :', '0093', 'cost = ', '0.313571403')\n",
      "('STEP :', '0094', 'cost = ', '0.138808477')\n",
      "('STEP :', '0095', 'cost = ', '0.252902637')\n",
      "('STEP :', '0096', 'cost = ', '0.116072721')\n",
      "('STEP :', '0097', 'cost = ', '0.122740490')\n",
      "('STEP :', '0098', 'cost = ', '0.110769906')\n",
      "('STEP :', '0099', 'cost = ', '0.300525235')\n",
      "('STEP :', '0100', 'cost = ', '0.232539348')\n",
      "('STEP :', '0101', 'cost = ', '0.195619466')\n",
      "('STEP :', '0102', 'cost = ', '0.316556208')\n",
      "('STEP :', '0103', 'cost = ', '0.287318205')\n",
      "('STEP :', '0104', 'cost = ', '0.139998428')\n",
      "('STEP :', '0105', 'cost = ', '0.262857374')\n",
      "('STEP :', '0106', 'cost = ', '0.118249996')\n",
      "('STEP :', '0107', 'cost = ', '0.150331747')\n",
      "('STEP :', '0108', 'cost = ', '0.117034932')\n",
      "('STEP :', '0109', 'cost = ', '0.266148701')\n",
      "('STEP :', '0110', 'cost = ', '0.273032931')\n",
      "('STEP :', '0111', 'cost = ', '0.119402454')\n",
      "('STEP :', '0112', 'cost = ', '0.392247312')\n",
      "('STEP :', '0113', 'cost = ', '0.465723060')\n",
      "('STEP :', '0114', 'cost = ', '0.112101697')\n",
      "('STEP :', '0115', 'cost = ', '0.179341471')\n",
      "('STEP :', '0116', 'cost = ', '1.059517659')\n",
      "('STEP :', '0117', 'cost = ', '0.286747266')\n",
      "('STEP :', '0118', 'cost = ', '0.324724642')\n",
      "('STEP :', '0119', 'cost = ', '0.116433620')\n",
      "('STEP :', '0120', 'cost = ', '2.048931433')\n",
      "('STEP :', '0121', 'cost = ', '0.250019118')\n",
      "('STEP :', '0122', 'cost = ', '0.216517562')\n",
      "('STEP :', '0123', 'cost = ', '0.319020045')\n",
      "('STEP :', '0124', 'cost = ', '0.125730036')\n",
      "('STEP :', '0125', 'cost = ', '0.143572602')\n",
      "('Average cost : ', 0.26792198346652024)\n"
     ]
    }
   ],
   "source": [
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(testSet2[i] - encode_decode[i], 2))\n",
    "    print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Train with normal data's 80 % and find anomally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempSet = trainingSet\n",
    "np.random.shuffle(tempSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainShuffledData = tempSet[:640000,:]\n",
    "testShuffled = tempSet[640000:780000,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Training data : ', (640000, 41))\n",
      "(' Test data     : ', (140000, 41))\n"
     ]
    }
   ],
   "source": [
    "print(\" Training data : \", trainShuffledData.shape)\n",
    "print(\" Test data     : \", testShuffled.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 2560)\n",
      "('Epoch:', '0001', 'cost=', '0.845439613')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.319008082')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.592409730')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '0.633674324')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.385230154')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '0.675249636')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '0.324178785')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.807040274')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '0.300024778')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.923783362')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.377356678')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 250\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 38 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainShuffledData.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainShuffledData.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainShuffledData[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average cost : ', 0.5258665931149764)\n",
      "('Testing for normal data ', 16479, ' anomaly detected ')\n",
      "('Number of detected Anomaly ', 16479)\n",
      "('Number of total anomaly data ', 140000)\n",
      "('anomally detection for normal data  percenteges is  ', 11.770714285714286)\n",
      "('Correct prediction is ', 88.22928571428571)\n"
     ]
    }
   ],
   "source": [
    "anomally =[]\n",
    "anomalNumber = 0\n",
    "\n",
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: testShuffled})\n",
    "\n",
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(testShuffled[i] - encode_decode[i], 2))\n",
    "    if tempCost > 0.5 :\n",
    "        anomally.append(i)\n",
    "        anomalNumber += 1\n",
    "    #print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)\n",
    "\n",
    "#for i in range(anomalNumber):\n",
    "#    print(\"Anomaly data index is : \", anomally[i] )\n",
    "\n",
    "print(\"Testing for normal data \", anomalNumber , \" anomaly detected \")\n",
    "correct =  (100 * float(anomalNumber) / float(testShuffled.shape[0]))\n",
    "print(\"Number of detected Anomaly \", anomalNumber)\n",
    "print(\"Number of total anomaly data \", testShuffled.shape[0])\n",
    "print(\"anomally detection for normal data  percenteges is  \", correct)\n",
    "correct = 100 - correct\n",
    "print(\"Correct prediction is \", correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training with normal data and detection anomaly by Dos test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Training data : ', (787953, 41))\n",
      "(' Test data     : ', (247256, 41))\n"
     ]
    }
   ],
   "source": [
    "tempSet = trainingSet\n",
    "np.random.shuffle(tempSet)\n",
    "trainShuffledData = tempSet\n",
    "dosTest = np.empty(shape=[dosLen, 41])\n",
    "for i in range(dosLen):\n",
    "    for j in range(41):\n",
    "        dosTest[i][j] = dosList[i][j] \n",
    "dosTest[:,19] = 0        \n",
    "print(\" Training data : \", trainShuffledData.shape)\n",
    "print(\" Test data     : \", dosTest.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3151)\n",
      "('Epoch:', '0001', 'cost=', '1.013197064')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.309251010')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.274771899')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '1.446210027')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.367053807')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '4.279848099')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '0.764706135')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.423874378')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '0.614094377')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.327837259')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.478001237')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 250\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 38 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainShuffledData.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainShuffledData.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainShuffledData[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average cost : ', 1.1068676201798393)\n",
      "('Testing for dos data ', 246291, ' anomal data detected ')\n",
      "('Number of detected Anomaly ', 246291)\n",
      "('Number of total anomaly data ', 247256)\n",
      "('anomally detection correction for dos data  percenteges is  ', 99.60971624551073)\n"
     ]
    }
   ],
   "source": [
    "anomally =[]\n",
    "anomalNumber = 0\n",
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: dosTest})\n",
    "\n",
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(dosTest[i] - encode_decode[i], 2))\n",
    "    if tempCost > 0.5 :\n",
    "        anomally.append(i)\n",
    "        anomalNumber += 1\n",
    "    #print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)\n",
    "\n",
    "#for i in range(anomalNumber):\n",
    "#    print(\"Anomaly data index is : \", anomally[i] )\n",
    "\n",
    "print(\"Testing for dos data \", anomalNumber , \" anomal data detected \")\n",
    "correct =  (100 * float(anomalNumber) / float(dosLen))\n",
    "print(\"Number of detected Anomaly \", anomalNumber)\n",
    "print(\"Number of total anomaly data \", dosLen)\n",
    "print(\"anomally detection correction for dos data  percenteges is  \", correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with normal data and detection anomaly by Probe test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Training data : ', (787953, 41))\n",
      "(' Test data     : ', (13295, 41))\n"
     ]
    }
   ],
   "source": [
    "tempSet = trainingSet\n",
    "np.random.shuffle(tempSet)\n",
    "trainShuffledData = tempSet\n",
    "probeTest = np.empty(shape=[probeLen, 41])\n",
    "for i in range(probeLen):\n",
    "    for j in range(41):\n",
    "        probeTest[i][j] = probeList[i][j] \n",
    "        \n",
    "print(\" Training data : \", trainShuffledData.shape)\n",
    "print(\" Test data     : \", probeTest.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3151)\n",
      "('Epoch:', '0001', 'cost=', '0.972607076')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.291858435')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.291937917')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '0.226983249')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.218233779')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '0.245217010')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '4.505098820')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.436607093')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '0.227027893')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.305429220')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.249904200')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 250\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 38 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainShuffledData.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainShuffledData.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainShuffledData[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average cost : ', 7.125895367296353)\n",
      "('Testing for probe data ', 13106, ' anomal data detected ')\n",
      "('Number of detected Anomaly ', 13106)\n",
      "('Number of total anomaly data ', 13295)\n",
      "('anomally detection correction for dos data  percenteges is  ', 98.57841293719443)\n"
     ]
    }
   ],
   "source": [
    "anomally =[]\n",
    "anomalNumber = 0\n",
    "\n",
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: probeTest})\n",
    "\n",
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(probeTest[i] - encode_decode[i], 2))\n",
    "    if tempCost > 0.4 :\n",
    "        anomally.append(i)\n",
    "        anomalNumber += 1\n",
    "    #print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)\n",
    "\n",
    "#for i in range(anomalNumber):\n",
    "#    print(\"Anomaly data index is : \", anomally[i] )\n",
    "\n",
    "print(\"Testing for probe data \", anomalNumber , \" anomal data detected \")\n",
    "correct =  (100 * float(anomalNumber) / float(probeTest.shape[0]))\n",
    "print(\"Number of detected Anomaly \", anomalNumber)\n",
    "print(\"Number of total anomaly data \", probeLen)\n",
    "print(\"anomally detection correction for dos data  percenteges is  \", correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with normal data and detection anomaly by u2r test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Training data : ', (787953, 41))\n",
      "(' Test data     : ', (35, 41))\n"
     ]
    }
   ],
   "source": [
    "tempSet = trainingSet\n",
    "np.random.shuffle(tempSet)\n",
    "trainShuffledData = tempSet\n",
    "u2rTest = np.empty(shape=[u2rLen, 41])\n",
    "for i in range(u2rLen):\n",
    "    for j in range(41):\n",
    "        u2rTest[i][j] = u2rList[i][j] \n",
    "u2rTest[:,19] = 0        \n",
    "print(\" Training data : \", trainShuffledData.shape)\n",
    "print(\" Test data     : \", u2rTest.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3151)\n",
      "('Epoch:', '0001', 'cost=', '1.382441759')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.297699720')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.311143458')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '0.314602822')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.832315147')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '0.346339673')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '0.544762552')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.361529738')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '0.668155849')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.295091957')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.203608111')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 250\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 38 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainShuffledData.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainShuffledData.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainShuffledData[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average cost : ', 48.00187496361766)\n",
      "('Testing for u2r data ', 32, ' anomal data detected ')\n",
      "('Number of detected Anomaly ', 32)\n",
      "('Number of total anomaly data ', 35)\n",
      "('anomally detection correction for u2r data  percenteges is  ', 91.42857142857143)\n"
     ]
    }
   ],
   "source": [
    "anomally =[]\n",
    "anomalNumber = 0\n",
    "\n",
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: u2rTest})\n",
    "\n",
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(u2rTest[i] - encode_decode[i], 2))\n",
    "    if tempCost > 0.4 :\n",
    "        anomally.append(i)\n",
    "        anomalNumber += 1\n",
    "    #print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)\n",
    "\n",
    "#for i in range(anomalNumber):\n",
    "#    print(\"Anomaly data index is : \", anomally[i] )\n",
    "\n",
    "print(\"Testing for u2r data \", anomalNumber , \" anomal data detected \")\n",
    "correct =  (100 * float(anomalNumber) / float(u2rLen))\n",
    "print(\"Number of detected Anomaly \", anomalNumber)\n",
    "print(\"Number of total anomaly data \", u2rLen)\n",
    "print(\"anomally detection correction for u2r data  percenteges is  \", correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with normal data and detection anomaly by r2l test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Training data : ', (787953, 41))\n",
      "(' Test data     : ', (36, 41))\n"
     ]
    }
   ],
   "source": [
    "tempSet = trainingSet\n",
    "np.random.shuffle(tempSet)\n",
    "trainShuffledData = tempSet\n",
    "r2lTest = np.empty(shape=[r2lLen, 41])\n",
    "for i in range(r2lLen):\n",
    "    for j in range(41):\n",
    "        r2lTest[i][j] = r2lList[i][j] \n",
    "r2lTest[:,19] = 0        \n",
    "print(\" Training data : \", trainShuffledData.shape)\n",
    "print(\" Test data     : \", r2lTest.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Total batch = ', 3151)\n",
      "('Epoch:', '0001', 'cost=', '0.961491048')\n",
      "('Step Count :', 0)\n",
      "('Epoch:', '0401', 'cost=', '0.251833797')\n",
      "('Step Count :', 400)\n",
      "('Epoch:', '0801', 'cost=', '0.496878624')\n",
      "('Step Count :', 800)\n",
      "('Epoch:', '1201', 'cost=', '0.281488627')\n",
      "('Step Count :', 1200)\n",
      "('Epoch:', '1601', 'cost=', '0.574708700')\n",
      "('Step Count :', 1600)\n",
      "('Epoch:', '2001', 'cost=', '0.304099053')\n",
      "('Step Count :', 2000)\n",
      "('Epoch:', '2401', 'cost=', '0.330498070')\n",
      "('Step Count :', 2400)\n",
      "('Epoch:', '2801', 'cost=', '0.503193319')\n",
      "('Step Count :', 2800)\n",
      "('Epoch:', '3201', 'cost=', '0.227206394')\n",
      "('Step Count :', 3200)\n",
      "('Epoch:', '3601', 'cost=', '0.225252435')\n",
      "('Step Count :', 3600)\n",
      "('Epoch:', '4001', 'cost=', '0.364628971')\n",
      "('Step Count :', 4000)\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 250\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 41\n",
    "n_hidden_1 = 38 # 1st layer num features\n",
    "n_hidden_2 = 35 # 2nd layer num features\n",
    "\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "display_step = 400\n",
    "num_steps=4001\n",
    "total_batch = int(trainShuffledData.shape[0]/batch_size)\n",
    "print(\"Total batch = \",total_batch)\n",
    "# Training cycle\n",
    "for step in range(num_steps):\n",
    "# Pick an offset within the training data, which has been randomized.\n",
    "# Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (trainShuffledData.shape[0])\n",
    "# Generate a minibatch.\n",
    "    batch_data = trainShuffledData[offset:(offset + batch_size), :]\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={X: batch_data})\n",
    "    if step % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (step+1),\"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Step Count :\" ,step);\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average cost : ', 31.198680513608863)\n",
      "('Testing for r2l data ', 33, ' anomal data detected ')\n",
      "('Number of detected Anomaly ', 33)\n",
      "('Number of total anomaly data ', 36)\n",
      "('anomally detection correction for r2l data  percenteges is  ', 94.28571428571429)\n"
     ]
    }
   ],
   "source": [
    "anomally =[]\n",
    "anomalNumber = 0\n",
    "\n",
    "#encodeDecode = sess.run(y_pred, feed_dict={X: testData[:10000]})\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: r2lTest})\n",
    "\n",
    "tempCost = 0\n",
    "totalCost= 0\n",
    "step = 0\n",
    "\n",
    "for i in range(encode_decode.shape[0]):\n",
    "    tempCost = np.mean(pow(r2lTest[i] - encode_decode[i], 2))\n",
    "    if tempCost > 0.5 :\n",
    "        anomally.append(i)\n",
    "        anomalNumber += 1\n",
    "    #print(\"STEP :\", '%04d' % (step+1),\"cost = \", \"{:.9f}\".format(tempCost))\n",
    "    step +=1\n",
    "    totalCost+=tempCost\n",
    "averageCost=float(totalCost/encode_decode.shape[0])\n",
    "print(\"Average cost : \", averageCost)\n",
    "\n",
    "#for i in range(anomalNumber):\n",
    "#    print(\"Anomaly data index is : \", anomally[i] )\n",
    "\n",
    "print(\"Testing for r2l data \", anomalNumber , \" anomal data detected \")\n",
    "correct =  (100 * float(anomalNumber) / float(u2rLen))\n",
    "print(\"Number of detected Anomaly \", anomalNumber)\n",
    "print(\"Number of total anomaly data \", r2lLen)\n",
    "print(\"anomally detection correction for r2l data  percenteges is  \", correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
